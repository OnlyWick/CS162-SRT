1
00:00:00,000 --> 00:00:29,840
Hello everyone.
大家好。

2
00:00:29,840 --> 00:00:34,160
Welcome. Today we are going to continue our discussion on file systems.
欢迎。今天我们将继续讨论文件系统。

3
00:00:34,160 --> 00:00:43,360
And if you remember last time, we are discussing about, we ended our lecture with a discussion of
而且如果你还记得上次，我们讨论的是，我们在讲座结束时进行了一次讨论。

4
00:00:43,360 --> 00:00:53,920
the fast file systems. And we are, which was the version, the next version of the file system in
快速文件系统。我们正在开发的是文件系统的下一个版本。

5
00:00:53,920 --> 00:01:04,880
Unix BSD after original version. And this has a lot of, quite a few innovations. And we went over
Unix BSD是原始版本之后的一个版本。它有很多创新，相当多的创新。我们已经讨论过了。

6
00:01:04,880 --> 00:01:16,160
a few last time, mostly the innovation or about improving the performance like different block
最近几次，主要是关于创新或改进性能的事情，比如不同的区块。

7
00:01:16,160 --> 00:01:26,560
sizes, you know, basically improving the rotation latency and a few more features to again, to
尺寸，你知道的，基本上是为了改善旋转延迟和一些其他功能，再次提升。

8
00:01:26,560 --> 00:01:33,600
improve the performance. The one thing we haven't, we didn't go over is the organization of the
提高性能。我们还没有讨论的一件事是组织结构。

9
00:01:33,600 --> 00:01:41,520
directories. So what is the, what is the problem here? So assume that you have a very large
目录。那么问题是什么呢？假设你有一个非常大的目录。

10
00:01:41,520 --> 00:01:49,600
directory with many entries. Now, if you remember the early file systems organized directories as
目录中有很多条目。现在，如果你还记得早期的文件系统是如何组织目录的，那么你会知道

11
00:01:49,600 --> 00:01:58,880
lists, this was in the case of FAT, the Microsoft DOS first file system,
列表，这是在FAT（文件分配表）的情况下，微软DOS的第一个文件系统。

12
00:01:58,880 --> 00:02:09,200
or arrays and of entries or an entry is a file name and the inode corresponding to that file name.
或者数组和条目，或者一个条目是一个文件名和对应于该文件名的inode。

13
00:02:10,640 --> 00:02:17,040
And I know if you remember contains the entire information about where,
我知道如果你记得的话，包含了关于在哪里的所有信息。

14
00:02:17,040 --> 00:02:24,960
which are the blocks of that particular file, as well as other information about that file, like
哪些是该特定文件的块，以及关于该文件的其他信息，比如

15
00:02:24,960 --> 00:02:32,640
access rights and things like that. The time was on last updated and other information.
访问权限和类似的事情。时间是在上次更新和其他信息上。

16
00:02:34,000 --> 00:02:40,880
So the challenge with that is that say, I want to locate one file, right? And whenever you are going
那么这个挑战就是，比如说，我想要定位一个文件，对吧？而每当你进行这个操作的时候，

17
00:02:40,880 --> 00:02:49,280
to, and there's a common line, or you are going to write a file names to say, see the contact to open,
对，有一个常见的方法，或者你可以写一个文件名来说明，查看联系人打开。

18
00:02:49,280 --> 00:02:56,720
you need to find that file name because in the directory, because the entry in the directory of
你需要找到那个文件名，因为在目录中，因为目录中的条目。

19
00:02:56,720 --> 00:03:02,080
that file name, because that entry will give you the inode and the inode will tell you where are
那个文件名，因为那个条目会给你inode，而inode会告诉你文件在哪里。

20
00:03:02,080 --> 00:03:10,320
the data blocks on the disk of that file. But if the directory is organized like a list or an
那个文件的磁盘上的数据块。但是如果目录像一个列表或一个

21
00:03:10,320 --> 00:03:18,800
array of entries, then the only way to locate that file is linear search. If the directory is very
如果目录是一个条目的数组，那么定位该文件的唯一方法就是线性搜索。如果目录非常大，这可能会导致性能问题。

22
00:03:18,800 --> 00:03:28,960
large, this linear search is expensive. Even more, you may need to read the entire directory just to
大的，这个线性搜索很耗费资源。更甚的是，你可能需要读取整个目录才能完成搜索。

23
00:03:28,960 --> 00:03:36,080
find the file. And again, if the file, if the directory is large, you may have to load all the
找到文件。而且，如果文件夹很大，你可能需要加载全部的文件。

24
00:03:36,080 --> 00:03:44,080
blocks which are storing this large directory, again, just to find the file. Okay. So what is
存储这个大目录的块，再次只是为了找到文件。好的。那么问题是什么？

25
00:03:44,080 --> 00:03:50,800
the solution here? Of course, the solution is to find a better structure than a list of an array.
这里的解决方案？当然，解决方案是找到一个比数组列表更好的结构。

26
00:03:51,600 --> 00:03:59,040
And if you want to search, what are some better structure to search than a list or an array?
如果你想进行搜索，除了列表或数组，还有哪些更好的数据结构可以使用呢？

27
00:03:59,040 --> 00:04:08,800
You learn about many of those. Can you give an example? What would you do if you were to design
您会了解其中许多。您能给一个例子吗？如果您要设计的话，您会怎么做？

28
00:04:08,800 --> 00:04:17,280
the directory data structure, which is much more efficient in terms of search? It's a binary tree,
目录数据结构中，搜索效率更高的是什么？是二叉树。

29
00:04:17,280 --> 00:04:28,080
right? Okay. Yeah, that's pretty good. And it's even better here, we can have a B-tree. Okay. And
对吗？好的。是的，这很不错。而且在这里，我们可以使用B树。好的。而且

30
00:04:28,080 --> 00:04:36,400
when you have a tree, a search tree, the depth of the tree is what gives you the complexity of
当你有一棵树，一棵搜索树时，树的深度决定了复杂度。

31
00:04:36,400 --> 00:04:47,040
locating that particular, in our case, file, which is a leaf in that tree. So this is for you,
定位那个特定的文件，在我们的情况下，它是树中的一个叶子。所以这是给你的，

32
00:04:47,040 --> 00:04:58,080
just a B-tree, it's a more sophisticated search tree. And if you remember, this is just a reminder
只是一个B树，它是一种更复杂的搜索树。如果你还记得的话，这只是一个提醒。

33
00:04:58,080 --> 00:05:12,240
for you to refresh your memory. So here, a node is pointing out at a lower level to an array. And
为了帮助你回忆一下，这里的一个节点指向一个较低级别的数组。而且

34
00:05:12,240 --> 00:05:19,680
that array contains values, which are smaller than the value of the pattern node. So in this case,
该数组包含的值小于模式节点的值。因此，在这种情况下，

35
00:05:19,680 --> 00:05:30,160
100 points to this array of values, and all these values 48, 50, 79 are less than 100.
这个数组中的值都是100分，而这些值48、50、79都小于100分。

36
00:05:30,160 --> 00:05:41,120
Also the following node, the sibling node of 100, 155, now is going to point to a set of values,
还有下一个节点，100和155的兄弟节点，现在将指向一组值，

37
00:05:41,120 --> 00:05:48,240
to an array of values, which are lower than itself, but higher than its sibling on the left,
转换为一个值数组，这些值比它自己小，但比左侧的兄弟节点大。

38
00:05:48,240 --> 00:05:55,920
right? So these are numbers between 128 and 140 and so forth. Okay.
对吗？所以这些是128到140之间的数字，依此类推。好的。

39
00:05:55,920 --> 00:06:05,440
Eventually, you need to get to the leafs and let's see how to find a value at the leaf.
最终，你需要到达叶子节点，让我们看看如何在叶子节点找到一个值。

40
00:06:06,080 --> 00:06:11,600
So let's see how this is used in the context of directories. So in the context of directories,
那么让我们看看在目录的背景下如何使用它。在目录的背景下，

41
00:06:11,600 --> 00:06:18,000
how do we get a number? Well, the number is easy to get, you just hash the name of the file. So now
我们如何获得一个数字？嗯，获取数字很简单，只需对文件名进行哈希处理即可。所以现在

42
00:06:18,000 --> 00:06:23,120
we get a number and hopefully the hash is good enough. So the probability of collisions is very
我们得到一个数字，希望哈希值足够好。因此碰撞的概率非常低。

43
00:06:23,120 --> 00:06:30,720
small. Okay. But this is what it is. It's exactly, it's a binary search tree, sorry, it's a B-tree,
小。好的。但这就是它的样子。确切地说，它是一个二叉搜索树，抱歉，是一个B树。

44
00:06:31,280 --> 00:06:40,800
in which the values in the B-trees are the hashes of the file names.
其中B树中的值是文件名的哈希值。

45
00:06:40,800 --> 00:06:51,280
And you can see here on the leafs, you are going to see to have the entries in the directory,
而且你可以在这些叶子上看到，在目录中你会看到有条目。

46
00:06:51,280 --> 00:06:58,880
which again is a name of the file, the name and the file number or inode.
再次，这是文件的名称，包括名称、文件号或inode。

47
00:07:01,040 --> 00:07:17,840
Any questions? Okay. So in this particular case, you see that the hash to out to it's C194,
有任何问题吗？好的。在这个特定的案例中，你可以看到哈希值是C194。

48
00:07:17,840 --> 00:07:23,920
and you are looking through the B-trees. First of all, you are going to look at the values,
然后你要浏览B树。首先，你要查看值，

49
00:07:24,640 --> 00:07:32,480
which are, you are looking to the values, to the consecutive values as that the value on the left
你是在寻找数值，寻找连续的数值，就像左边的数值一样。

50
00:07:32,480 --> 00:07:41,600
is lower, the value on the right is higher than C194. And then you are going to pick the node
较低的是，右侧的值高于C194。然后你将选择该节点。

51
00:07:41,600 --> 00:07:51,120
corresponding to that, which is for the range. And then you are going to go on level down. So for
相应于那个，那是用于范围的。然后你要继续往下一级。所以对于...

52
00:07:51,120 --> 00:07:59,040
instance, in this case, C194 is smaller than the first value at the first level. So you are going
在这种情况下，C194比第一层级的第一个值要小。所以你是正确的。

53
00:07:59,040 --> 00:08:13,520
to go take and go to the arrays pointed out by that value node, AD1102. Okay. Because remember,
去获取并前往由该值节点AD1102指出的数组。好的。因为记住，

54
00:08:13,520 --> 00:08:23,440
AD1102, which is a leftmost node here at the top level is going to point only to values which are
AD1102，在这里是顶层的最左节点，将只指向值。

55
00:08:23,440 --> 00:08:31,280
lower than itself. Okay. So you are going to go to the next level, the next level, the second level.
比它自己低。好的。所以你要进入下一个层次，下一个层次，第二个层次。

56
00:08:31,280 --> 00:08:40,160
Again, you are going to look at the values at that node, and the values are going to go from
再次，你要查看该节点的值，并且这些值将会从...开始。

57
00:08:40,160 --> 00:08:48,800
the smallest value is C195, which again C195 is greater than C194. So you are going to go again
最小的值是C195，而C195比C194大。所以你要再次进行。

58
00:08:48,800 --> 00:08:56,560
to the next level, it's going to be a node, which is pointed out by the first,
到下一个级别，它将成为一个节点，由第一个指出。

59
00:08:56,560 --> 00:09:04,960
corresponding to the leftmost value. Okay. And finally here, you are just going to see that
对应于最左边的值。好的。最后，在这里，你只会看到

60
00:09:05,520 --> 00:09:11,600
you are going to have to find C194, you'll find a match, and that match will take you to the
你需要找到C194，你会找到一个匹配项，而这个匹配项将带领你去到...

61
00:09:11,600 --> 00:09:18,400
entry in the file directory, which is the name of the file and the file number.
文件目录中的条目，包括文件名和文件编号。

62
00:09:18,400 --> 00:09:32,160
Okay. So Michael here has a great question. Why do we want to hash the file names? Wouldn't using
好的。所以迈克尔在这里有一个很好的问题。为什么我们要对文件名进行哈希处理？使用哈希处理的话，不是会导致文件名无法识别吗？

63
00:09:32,160 --> 00:09:38,240
the file name themselves in lexicographical order have the benefit that it's easy to enumerate files
文件名按字典顺序自身排序的好处是很容易枚举文件。

64
00:09:38,240 --> 00:09:49,920
in lexicographical order? Anyone wants to answer this question?
按字典顺序排列吗？有人想回答这个问题吗？

65
00:09:49,920 --> 00:10:09,520
Yes. The file names can have arbitrary length, so it's going to make the implementation a little bit
是的。文件名可以有任意长度，所以这会使实现变得有点复杂。

66
00:10:09,520 --> 00:10:17,760
harder. Someone says also perhaps to distribute the hash is uniform, that's a great point,
更难。有人还说，也许将哈希分布均匀是一个很好的观点。

67
00:10:18,640 --> 00:10:26,800
as well. So you get a more balanced tree. If you have a hash, a good hash is going to give
以及。这样你就会得到一棵更加平衡的树。如果你有一个哈希表，一个好的哈希函数会给出一个好的哈希值。

68
00:10:26,800 --> 00:10:33,200
you pseudo random numbers. So you are going to easily get a balanced tree once you build it.
你是伪随机数。所以一旦你构建它，你将很容易得到一棵平衡树。

69
00:10:33,200 --> 00:10:39,360
But the other thing is that if you use a file names, it's strings. So comparing two strings
但另一个问题是，如果你使用文件名，它们是字符串。因此，比较两个字符串。

70
00:10:39,360 --> 00:10:46,560
is much more expensive than comparing two numbers. Okay. So again, remember we are doing building
是比较两个数字要贵得多。好的。所以再次提醒，我们正在进行建筑。

71
00:10:46,560 --> 00:10:53,120
bit trees because we want to improve the performance. So hashes give you values,
位树是因为我们想要提高性能。所以哈希函数会给出数值，

72
00:10:53,120 --> 00:11:00,160
which at the end of the day can be our integers. And just comparing integers is much easier,
最终可以成为我们的整数。而且仅仅比较整数要容易得多。

73
00:11:00,160 --> 00:11:08,240
it's much faster and also much easier to build data structures where the fields have
它的速度更快，而且构建字段具有的数据结构也更容易。

74
00:11:09,040 --> 00:11:19,440
unique with the same length. Do different file systems use different bit tree types or is some
唯一且长度相同。不同的文件系统是否使用不同的位树类型，还是有一些共同的？

75
00:11:19,440 --> 00:11:26,400
standard like 2-3-4? That's a great question. Actually, I do not know the answer to that
标准像2-3-4这样吗？这是一个很好的问题。实际上，我不知道答案。

76
00:11:26,400 --> 00:11:35,120
question. I should look around. But it's a good question. I would assume that there are some
问题。我应该四处看看。但这是一个好问题。我会假设有一些。

77
00:11:36,000 --> 00:11:39,440
rule of thumb here about what are the good numbers for this application.
这里有一个经验法则，关于这个应用程序的好数字是什么。

78
00:11:39,440 --> 00:11:49,840
While directory can be large, it doesn't compare with the number of rows you can have in a table.
虽然目录可能很大，但与表中的行数相比，它并不算什么。

79
00:11:49,840 --> 00:11:56,560
As you probably know, bit trees are coming from database field to index the rows in a table.
正如你可能知道的那样，位树是从数据库字段中提取出来的，用于对表中的行进行索引。

80
00:11:56,560 --> 00:12:02,400
So I guess you can have hundreds of millions or even billions of rows in a table.
所以我猜你可以在一个表中有数亿甚至数十亿行。

81
00:12:02,960 --> 00:12:08,160
I don't know the number of files can be large is not going to reach that value.
我不知道文件的数量是否会很大，是否会达到那个值。

82
00:12:08,160 --> 00:12:19,360
But yeah, that's a good question. Okay. So now we switch gears and we are going to look at
但是，是的，这是一个很好的问题。好的。现在我们换个角度，我们要来看一下...

83
00:12:19,360 --> 00:12:26,480
another file system and file system is Windows NTFS. What NTFS stands for?
另一个文件系统和文件系统是Windows NTFS。NTFS代表什么？

84
00:12:29,120 --> 00:12:38,080
New technology. So it's called New Technology File System. It's an interesting name because
新技术。所以它被称为新技术文件系统。这是一个有趣的名字，因为

85
00:12:38,080 --> 00:12:44,880
it's always going to be called new technology. But of course, now this is like 30, 20, 30 years old.
它总是被称为新技术。但当然，现在这已经是30、20、30年前的事了。

86
00:12:44,880 --> 00:12:53,920
So it cannot be that so new technology. And this is still the default on modern Windows systems.
所以这不可能是如此新的技术。而且这仍然是现代Windows系统的默认设置。

87
00:12:54,560 --> 00:13:02,080
And if you are using a Windows machine, most likely you are using an NTFS file system.
如果你使用的是Windows机器，很可能你正在使用NTFS文件系统。

88
00:13:02,080 --> 00:13:10,080
And let me just tell you what is the main idea here. The main idea here, one of the main ideas
is to promote cultural exchange and understanding between different countries. This can be achieved through various means such as language learning programs, cultural festivals, and international collaborations in fields like art, music, and sports. By fostering mutual respect and appreciation for each other's cultures, we can bridge the gaps and build stronger connections among nations. Additionally, this initiative aims to break down stereotypes and prejudices, promoting a more inclusive and harmonious global community.

89
00:13:10,080 --> 00:13:21,840
is that instead of fixed blocks, we have variable length blocks. And these variable length blocks
是的，我们使用可变长度的块而不是固定长度的块。而且这些可变长度的块

90
00:13:21,840 --> 00:13:34,240
are called extents. Okay. And then instead of the FAT or INOD array, we are having here what is
被称为extents。好的。然后，我们这里不再使用FAT或INOD数组，而是使用的是什么？

91
00:13:34,240 --> 00:13:41,040
called a master file table. And the master file table is almost like a database. And it's in the
称为主文件表。主文件表几乎就像一个数据库。它位于...

92
00:13:41,040 --> 00:13:49,920
database because it has attribute value pairs and these attributes and the values can be different
数据库，因为它具有属性值对，而这些属性和值可以是不同的。

93
00:13:49,920 --> 00:14:01,200
things. So and each of these entries, which is attribute value, can have one kilobyte size.
事物。所以每个条目，也就是属性值，可以有1千字节的大小。

94
00:14:01,200 --> 00:14:11,440
So it's more general than the name, file name pairs, which you are finding in traditional file
所以它比传统文件中找到的名称、文件名对更通用。

95
00:14:11,440 --> 00:14:24,160
systems like we learn so far. And let me show you a little bit how this is organized. But the point
我们迄今为止学习的系统。让我给你展示一下这是如何组织的。但重点是什么？

96
00:14:24,160 --> 00:14:34,240
is that the reason it's more complex, but this complexity buys you efficiency and performance.
这就是为什么它更复杂，但这种复杂性能够带来效率和性能的原因。

97
00:14:34,960 --> 00:14:49,040
And you'll see how. One is if you can find enough room on the disk and you can have an extent,
然后你会看到如何操作。一种方法是如果你能在磁盘上找到足够的空间并且可以获得一个扩展。

98
00:14:49,040 --> 00:14:59,760
let's say of 100 kilobytes larger than any block. So 100 kilobytes extend or even higher
假设有一个比任何块都大100千字节的块。所以100千字节扩展或者更大。

99
00:14:59,760 --> 00:15:10,000
and just contiguous. Right. So it's very fast to read that block. There is no seek that extent.
并且是连续的。对的。所以读取该块非常快速。没有寻找该范围的过程。

100
00:15:10,000 --> 00:15:16,000
There is no seek time. You have only one seek time you need to waste in order to get to that
没有寻道时间。你只需要浪费一次寻道时间才能到达那里。

101
00:15:16,000 --> 00:15:24,400
extent. But then you read 100 kilobytes. With blocks, it's again, it's like they may not be
程度。但是接着你读了100千字节。使用块，又回到了，就像它们可能不是

102
00:15:24,400 --> 00:15:35,120
contiguous. So you are going to have more seek times and it's more complex. And not only that,
连续的。所以你会有更多的寻道时间，而且更复杂。不仅如此，

103
00:15:35,120 --> 00:15:41,520
but as you'll see, like for instance, if you have a small file which is more than one kilobyte,
但是你会发现，比如说，如果你有一个小于1千字节的文件，

104
00:15:41,520 --> 00:15:46,640
you can actually put in this master file table because one of these entries, you remember,
你实际上可以将它放入主文件表中，因为其中一个条目，你记得的。

105
00:15:48,000 --> 00:15:58,640
it can be up to one kilobyte. OK, and let's look. So this is a master file table
它可以达到一千字节。好的，让我们来看看。所以这是一个主文件表。

106
00:15:58,640 --> 00:16:07,600
and then you have a bunch of records and these records are content depends on how large is a file.
然后你会有一堆记录，这些记录的内容取决于文件的大小。

107
00:16:07,600 --> 00:16:14,160
You also are going to learn next lecture about journaling, which is a way to provide
你也将在下一堂课中学习有关日记写作的内容，这是一种提供

108
00:16:14,160 --> 00:16:22,320
file reliability for the files. OK, so this is a very small, it's a small file, a tiny file.
文件的可靠性对于这些文件来说非常重要。好的，所以这是一个非常小的文件，一个微小的文件。

109
00:16:22,320 --> 00:16:29,360
So the file is less than one kilobyte. So one of these entries is one kilobyte. So what do you have
所以这个文件小于一千字节。所以这些条目中的一个是一千字节。那么你有什么呢？

110
00:16:29,360 --> 00:16:35,280
here? You have in one of these entries in the master file table for these small files, you have
这里吗？你在主文件表中有一个条目，针对这些小文件之一，你有

111
00:16:35,280 --> 00:16:42,800
standard information, which is the usual, you know, that when the file was created or modified
标准信息，就是通常的，你知道的，就是文件的创建或修改时间。

112
00:16:42,800 --> 00:16:50,720
the last time, maybe you want to access the last time, owner ID, some security specifiers, flags,
上一次，也许你想要访问上一次的时间，所有者ID，一些安全限定符和标志。

113
00:16:50,720 --> 00:16:56,080
whether it's read only, hidden, whether you can see, like if you do like a user,
无论是只读、隐藏，还是你能否看到，就像你像一个用户一样。

114
00:16:56,080 --> 00:17:01,760
Alice, whether you see the file or not, and or it's a system file and things like that. Right.
Alice，无论你是否看到这个文件，或者它是一个系统文件之类的东西。对吧。

115
00:17:01,760 --> 00:17:05,680
So this is what you have in the standard information. Then you have the file name.
所以这是您在标准信息中所拥有的内容。然后您有文件名。

116
00:17:05,680 --> 00:17:12,000
OK, so in the file table, master file table, you have the file name. Remember that
好的，在文件表和主文件表中，你有文件名。记住这一点。

117
00:17:12,000 --> 00:17:18,400
in the case of inode array, you do not have, you have only inode, the inode, which is the file
在inode数组的情况下，你没有数组，你只有inode，inode就是文件。

118
00:17:18,400 --> 00:17:24,560
number. The file name is in the directory only. And then like we discussed, if the data is a
数字。文件名只在目录中。然后就像我们讨论过的那样，如果数据是...

119
00:17:24,560 --> 00:17:31,040
file is small, you may be able to put the entire data in this entry. Right. And then it's free.
文件很小，你可能能够将整个数据放入这个条目中。对的。然后它是免费的。

120
00:17:31,040 --> 00:17:39,600
And you are going to just read all one of these, all this record in one go. Right. So you're with
我会一口气读完这些话，这些记录。对吧。所以你是和我一起的。

121
00:17:39,600 --> 00:17:45,600
one disk access. You can read everything about that file, including the data. So it's as fast
一次磁盘访问。您可以读取有关该文件的所有内容，包括数据。因此，速度很快。

122
00:17:45,600 --> 00:17:54,960
as you can get. Right. So now what happens if the file is a little bit larger? If the file is larger,
就像你所能理解的一样。对的。那么如果文件稍微大一点会发生什么呢？如果文件较大，

123
00:17:54,960 --> 00:18:02,240
then instead of having the data here, right, you have a bunch of pointers, attribute values.
那么，不是将数据放在这里，而是有一堆指针和属性值。

124
00:18:02,240 --> 00:18:08,720
Remember the same attribute values. So you are going to have and each of those is going to point
记住相同的属性值。所以你将会有，并且每一个都将指向

125
00:18:08,720 --> 00:18:18,000
out to a data extent. Again, remember data extent, it's a variable size block. So in order to refer
到一个数据范围之外。再次强调，数据范围是一个可变大小的块。因此，为了引用

126
00:18:18,000 --> 00:18:29,360
to a variable size block or extent, you are going to have a start, a starting pointer and the length
对于一个可变大小的块或范围，你需要一个起始点，一个起始指针和长度。

127
00:18:29,360 --> 00:18:40,560
of the extent. Right. That's enough. Okay. So this is what you have. Now, if you have other even
的程度。对的。够了。好的。所以这就是你所拥有的。现在，如果你还有其他的

128
00:18:40,560 --> 00:18:51,280
larger files, then what you have, you can have pointers to other extents. Remember, to the other
更大的文件，比你现在拥有的，你可以有指向其他范围的指针。记住，指向其他范围的指针。

129
00:18:51,280 --> 00:19:02,080
MTF records. So and remember at the end of the day, extent is a contiguous region on the disk.
MTF记录。所以请记住，最终，extent是磁盘上的连续区域。

130
00:19:02,080 --> 00:19:12,880
And so these are, you know, this leaf now, so to speak, in this hierarchy are where the data
而这些，你知道的，这片叶子现在，可以说，在这个层次结构中，就是数据所在的地方。

131
00:19:12,880 --> 00:19:20,960
extends. Okay. So in this particular case, the attribute list is going to point to have to point
extends. 好的。所以在这种特殊情况下，属性列表将指向必须指向。

132
00:19:20,960 --> 00:19:38,720
to different entries in the master file table. And this is for a huge file. It's again, you are going
将不同的条目写入主文件表。而且这是针对一个巨大的文件。再次强调，你正在进行的是

133
00:19:38,720 --> 00:19:51,200
to have a lot of more empty MFT records. So here we go. Right. So basically at one level to summarize,
拥有更多的空MFT记录。所以我们开始吧。对的。基本上，总结一下，

134
00:19:51,200 --> 00:20:03,040
if the file is very small, the entire file can fit into an MFT record. And if the file is a little
如果文件非常小，整个文件可以适应一个MFT记录。而如果文件稍大一些，它将被存储在MFT记录之外。

135
00:20:03,040 --> 00:20:13,360
bit larger, then you still have on one of the MFT record, but instead of data, you have a bunch of
稍微大一点，然后你仍然有一个MFT记录，但是里面不是数据，而是一堆

136
00:20:13,360 --> 00:20:22,320
pointers to data extents. So it's one level of indirection. Right. And the extents are referred
数据范围的指针。所以这是一级间接。对的。而这些范围被引用

137
00:20:22,320 --> 00:20:32,480
by the start and the length. If the file is even larger, then you are going to have multiple MFT
通过起始位置和长度来确定。如果文件更大，那么你将会有多个MFT。

138
00:20:32,480 --> 00:20:38,720
records. And if the file is even larger, you are going to have more MFT records.
记录。如果文件更大，你将会有更多的MFT记录。

139
00:20:38,720 --> 00:20:49,920
Okay. Directories are implemented as bit trees like we learn. The file numbers identify its
Okay. 目录被实现为位树，就像我们学到的那样。文件号用于标识其

140
00:20:49,920 --> 00:20:59,360
entry in MFT. It is similar with the inode table or array. And MFT entry always has a file name
MFT中的条目。它类似于inode表或数组。而MFT条目总是有一个文件名。

141
00:20:59,360 --> 00:21:09,600
attribute. Right. Remember here, right. So you can check it. It's human readable as well. How do you
如何使用属性。对的。记住这里，对的。这样你就可以检查它。它也是可读的。你如何使用它？

142
00:21:09,600 --> 00:21:16,240
implement hardlink? Very simple. The way you are going to implement a hardlink, that will be another
实现硬链接？非常简单。你要实现硬链接的方式，那将是另一种方法。

143
00:21:16,240 --> 00:21:31,120
entry in the MFT table, which is going to point to the same data extents or is going to point to
MFT表中的条目，将指向相同的数据范围或将指向

144
00:21:31,120 --> 00:21:42,640
another MFT which represents the file. It's attribute list part of the data. This was a
另一个MFT代表着这个文件。它的属性列表是数据的一部分。这是一个

145
00:21:42,640 --> 00:21:50,320
question. Very good. I assume that where it's attribute list, I think it's next.
问题。非常好。我假设你在问属性列表在哪里，我认为它在下面。

146
00:21:50,320 --> 00:21:59,360
Yes. The attribute list, it's again, it's part of the MFT record.
是的。属性列表，它再次出现，它是MFT记录的一部分。

147
00:21:59,360 --> 00:22:07,040
I'm not sure what you mean, what the question means about asking about whether it's part of
我不确定你的意思，关于询问是否属于的问题是什么意思。

148
00:22:07,040 --> 00:22:15,760
the data. It's part of an MFT record for that file. And it's again, you have this attribute
数据。这是该文件的MFT记录的一部分。而且，你有这个属性。

149
00:22:15,760 --> 00:22:28,000
list if you cannot fit everything in a single MFT record, where instead of data, you are going
如果无法将所有内容放入单个MFT记录中，请列出您所在的位置，而不是数据。

150
00:22:28,000 --> 00:22:36,720
to have pointers to the data extents. So you cannot have this one. So here in the data part,
在数据部分，由于需要指向数据范围的指针，所以你不能有这个。

151
00:22:37,280 --> 00:22:44,160
you do not have enough room to store the pointers to all extents.
你没有足够的空间来存储所有范围的指针。

152
00:22:44,160 --> 00:23:04,240
Okay, that's great. So it's a question here from Akshay. What are the pros and cons of this,
好的，没问题。这是Akshay提出的一个问题。这个的优点和缺点是什么呢？

153
00:23:04,240 --> 00:23:13,840
I assume NTFS over Berkeley FAST file system? The list of extents seems similar to the list of
我假设选择NTFS而不是Berkeley FAST文件系统？范围列表似乎与列表相似

154
00:23:13,840 --> 00:23:21,680
direct and direct pointers in the inode. So the data part is replaced with an attribute list of
直接指针和间接指针在索引节点中。因此，数据部分被替换为属性列表。

155
00:23:21,680 --> 00:23:31,200
larger files. Yeah. Do you want to, anyone would like to answer?
更大的文件。是的。你想要，有人愿意回答吗？

156
00:23:31,200 --> 00:23:44,560
So Gilbert, you mentioned about the data part is replaced with the attribute list of
那么Gilbert，你提到数据部分被替换为属性列表。

157
00:23:44,560 --> 00:23:48,720
four large files. I'm not sure is that an answer to Akshay's question?
四个大文件。我不确定这是否是对Akshay问题的回答。

158
00:23:58,080 --> 00:24:04,400
So anyway, who would like to try to answer this Akshay question?
所以，谁愿意尝试回答这个Akshay的问题呢？

159
00:24:04,400 --> 00:24:18,080
What are the pros and cons between NTFS and FFS?
NTFS和FFS之间的优缺点是什么？

160
00:24:22,080 --> 00:24:42,080
So one thing is about, remember, what is the difference between extent and blocks reviews in FFS?
所以有一件事情是关于FFS中extent和blocks reviews之间的区别，记住了吗？

161
00:24:50,240 --> 00:24:56,480
Okay, so with extents, you can have any size blocks.
好的，所以使用extents，你可以拥有任意大小的块。

162
00:24:56,480 --> 00:25:04,240
What FFS gives you is the ability to configure the file system with different block sizes.
FFS给你的是能够使用不同的块大小配置文件系统的能力。

163
00:25:04,240 --> 00:25:07,920
But once you configure the block sizes will still be the same,
但是一旦你配置了块大小，它们仍然会保持不变。

164
00:25:07,920 --> 00:25:11,360
the same size, all the blocks are going to be the same size.
相同尺寸，所有的方块都将是相同的尺寸。

165
00:25:13,760 --> 00:25:22,960
Right? And with a data extent, with arbitrary size, you are going to be guaranteed that in
对吗？而且，通过数据范围，无论大小如何，你都能确保在其中。

166
00:25:22,960 --> 00:25:28,720
order to retrieve and to read all the data from one extent, you are going to have only need,
为了检索和阅读一个extent中的所有数据，你只需要一个需求。

167
00:25:28,720 --> 00:25:33,440
only one access, only one SIC to get access to that data.
只有一个访问权限，只有一个SIC可以访问那些数据。

168
00:25:33,440 --> 00:25:42,560
The blocks are going to be in general smaller than one extent. So you are going to have multiple
这些块通常会比一个尺寸小。所以你会有多个块。

169
00:25:42,560 --> 00:25:47,120
blocks and multiple blocks means that you may have to, if they are not contiguous and you are
块和多个块意味着如果它们不是连续的，你可能需要...

170
00:25:47,120 --> 00:25:53,840
not guaranteed to be contiguous if they are blocks, you may have to pay multiple access times to
如果它们是块，不能保证它们是连续的，你可能需要支付多次访问时间。

171
00:25:53,840 --> 00:26:03,920
read the same amount of data. NFS is also better for small files, exactly, Michael.
读取相同数量的数据。NFS对于小文件也更好，确切地说，Michael。

172
00:26:06,720 --> 00:26:16,400
And because like you see here, for if the file is very small, it's less than one kilobyte,
而且就像你在这里看到的一样，如果文件非常小，小于1千字节，

173
00:26:16,400 --> 00:26:27,440
then it fits in one MFT record. Well, in the previous case, you need to have multiple accesses
那么它适合放在一个MFT记录中。嗯，在前面的情况下，你需要进行多次访问。

174
00:26:27,440 --> 00:26:34,800
to load a small file, at least, you know, you need to go to read from inode,
加载一个小文件，至少，你知道，你需要从inode中读取。

175
00:26:35,760 --> 00:26:42,560
you need to go to read the data block, which is separate. Like this is equivalent,
你需要去读取数据块，这是分开的。就像这样是等价的。

176
00:26:42,560 --> 00:26:51,360
like the data itself will be part of the inode. Right? But with FFS is not. So you need to read
就像数据本身将成为inode的一部分一样。对吗？但是在FFS中不是这样的。所以你需要读取

177
00:26:51,360 --> 00:26:58,400
the inode, which is access. And then even if you don't have very tiny file, you still need to read
inode，它是访问。即使你没有非常小的文件，你仍然需要读取。

178
00:26:58,400 --> 00:27:13,200
another block where the data is located. Okay. Okay, excellent. Okay, now let's move and do
另一个区块，数据所在的地方。好的。好的，非常好。好的，现在让我们继续并进行...

179
00:27:13,200 --> 00:27:22,640
something even more interesting. Memory mapped files. And let's, what are the memory map files?
更有趣的是，内存映射文件。那么，什么是内存映射文件呢？

180
00:27:23,200 --> 00:27:29,440
You see, and why do we need those? You see, when you access a file, and what we learn,
你看，我们为什么需要这些呢？你看，当你访问一个文件时，我们学到了什么。

181
00:27:29,440 --> 00:27:35,440
things are getting quite complicated. And the data is replicated multiple places from that file,
事情变得相当复杂了。而且数据从那个文件中被复制到了多个地方。

182
00:27:35,440 --> 00:27:40,160
you need to the operating system has to have a buffer, we are going to learn more about the
你需要的操作系统必须有一个缓冲区，我们将会更多地了解它。

183
00:27:40,160 --> 00:27:45,280
buffer a little bit later, to store the data in that buffer. And then there is another buffer to
稍后缓冲一下，将数据存储在该缓冲区中。然后还有另一个缓冲区来

184
00:27:45,280 --> 00:27:51,680
put the data at the application level. Okay, so you have multiple copies of the data plus system
将数据放在应用程序级别。好的，所以你有多个数据副本加上系统。

185
00:27:51,680 --> 00:28:02,800
costs. Okay. But what if we could map an entire file directly to the memory? Right? And when you
成本。好的。但是如果我们能够直接将整个文件映射到内存中呢？对吧？而且当你

186
00:28:02,800 --> 00:28:09,840
are going to read something from that file, it just is like you read from that region of memory.
你将要从那个文件中读取一些内容，就像你从那个内存区域中读取一样。

187
00:28:09,840 --> 00:28:16,880
And when you have to write it, you write it like in memory. And eventually the operating system
而当你需要写入时，你会像在内存中一样进行写入。最终，操作系统会完成这个任务。

188
00:28:16,880 --> 00:28:26,720
will take care to write back your changes to the disk. Right? By the way, the executable files are
会将您的更改写回磁盘。对吗？顺便说一下，可执行文件是什么？

189
00:28:26,720 --> 00:28:35,920
treated this way when we are executing a process. So now, to see the differences, let's refresh our
当我们执行一个过程时，会以这种方式对待。所以现在，为了看到区别，让我们刷新一下我们的

190
00:28:35,920 --> 00:28:44,960
memory about what happens on a page fault. So you have an instruction, the instruction provides a
关于页面错误发生的记忆。所以你有一条指令，这条指令提供了一个

191
00:28:44,960 --> 00:28:53,360
virtual address, the virtual address goes through MMU and you are going to get the physical page
虚拟地址，虚拟地址经过内存管理单元（MMU）后，你将获得物理页。

192
00:28:53,360 --> 00:29:01,440
number. And then you are going to put the page number, the physical page number is going to index
数字。然后你要放置页码，物理页码将被索引。

193
00:29:01,440 --> 00:29:09,680
into a page table. Right? And the page table is going to give you the frame number.
将其转换为页表。对吗？而页表将提供给您帧号码。

194
00:29:10,640 --> 00:29:17,520
And the frame number is going to be, depending whether you are going to have paging or segmentation,
而帧号将取决于您是否使用分页或分段。

195
00:29:17,520 --> 00:29:22,080
you are going to concatenate the frame number with the offset and you are going to get the address
你将会将帧号与偏移量连接起来，从而得到地址。

196
00:29:22,080 --> 00:29:28,400
of the physical number. Okay. So you go virtual address, you get the page number from the virtual
地址，然后通过页表映射获取物理页号。

197
00:29:28,400 --> 00:29:33,600
address, it's going to point in the page table, it's going to give you the frame number, and then
地址，它将指向页表，它将给出帧号，然后

198
00:29:33,600 --> 00:29:39,280
you are going to get the offset of the virtual address and you are going to get the frame and
你将获得虚拟地址的偏移量，并且你将获得帧和...

199
00:29:39,280 --> 00:29:46,400
offset concatenating, it's going to give you the address in the physical memory. Right?
偏移拼接，它将给出物理内存中的地址。对吗？

200
00:29:46,400 --> 00:29:54,560
Now, if you are going to have a page fault, page fault means that you do not find the entry
现在，如果你要发生页面错误，页面错误意味着你找不到该条目。

201
00:29:54,560 --> 00:29:59,680
corresponding to the page number in the page table. So what we are going to do is there is an
对应于页表中的页码。所以我们要做的是有一个

202
00:29:59,680 --> 00:30:04,880
exception, which is going to be handled by the operating system in particular by the page fault
异常，将由操作系统处理，特别是由页面错误处理。

203
00:30:04,880 --> 00:30:14,560
handler. And that is going to locate on the disk what the page and is going to load it in memory.
处理程序。它将在磁盘上定位页面，并将其加载到内存中。

204
00:30:14,560 --> 00:30:21,040
Maybe for loading that page, you need to pick an existing page because maybe the memory is full.
也许加载该页面时，你需要选择一个已存在的页面，因为可能内存已满。

205
00:30:21,040 --> 00:30:29,120
And then you are going to update the page table to point to the page, which you just
然后你要更新页面表，将其指向刚刚的页面。

206
00:30:29,120 --> 00:30:37,520
loaded in memory. And then you return from the page fault and the operating system will reschedule
内存中已加载。然后你从页面错误返回，操作系统将重新调度。

207
00:30:37,520 --> 00:30:43,760
that process, which causes a page fault, re-execute the same instruction. And when
该过程会导致页面错误，然后重新执行相同的指令。而当

208
00:30:43,760 --> 00:30:50,560
you re-execute the same instruction, now you do have page table is correct. So you are going to
重新执行相同的指令，现在你确实拥有正确的页表。所以你将要...

209
00:30:50,560 --> 00:30:54,640
be able to read the page from the physical memory.
能够从物理内存中读取页面。

210
00:30:57,200 --> 00:31:03,280
OK, so we did that in the past. This is just a refresher. So now what happens is the memory
好的，所以我们过去做过这个。这只是一个复习。现在发生的是记忆。

211
00:31:03,280 --> 00:31:09,760
map files. Well, very simple. It's exactly what you would expect. Here is a file on the disk.
地图文件。嗯，非常简单。它就是你所期望的那样。这是磁盘上的一个文件。

212
00:31:09,760 --> 00:31:16,720
So you are going just to map the file in the memory and you are going to map to initialize
所以你只是要将文件映射到内存中，并且你要映射以进行初始化。

213
00:31:16,720 --> 00:31:22,400
the page table. You are going to allocate page table entries for each page of the file.
页面表。您将为文件的每个页面分配页面表项。

214
00:31:24,880 --> 00:31:35,280
And then when you have in this case a page fault, basically when you are going that the page is not
然后当你在这种情况下发生页面错误时，基本上是当你正在访问的页面不存在。

215
00:31:35,280 --> 00:31:42,240
in memory, which represent a file, which is part of the file, then you are going to load directly
在内存中，它代表一个文件，该文件是文件的一部分，然后你将直接加载它。

216
00:31:42,240 --> 00:31:53,920
the file, the portion of the file into the page. And now you are just going to read files when you
文件，文件的部分进入页面。现在你只需要在读取文件时。

217
00:31:53,920 --> 00:31:59,280
access it, when you access a physical memory, you basically access the content of the file.
访问它时，当你访问物理内存时，实际上是在访问文件的内容。

218
00:31:59,280 --> 00:32:10,480
OK. So this is a and I'll give you an example, some of which will clarify things.
好的。所以这是一个例子，我会给你一个例子，其中一些会澄清事情。

219
00:32:10,480 --> 00:32:20,240
This is a system call to create a memory map file. And this basically you provide the file descriptor
这是一个用于创建内存映射文件的系统调用。基本上，您需要提供文件描述符。

220
00:32:20,240 --> 00:32:26,880
of the file and the offset from where you want to get to map the data from the file. Right.
文件的路径和您希望从文件中映射数据的偏移量。对的。

221
00:32:26,880 --> 00:32:37,520
So, for instance, if offset is 100, this means that I want to map starting with a byte 100
所以，例如，如果偏移量是100，这意味着我想从第100个字节开始进行映射。

222
00:32:37,520 --> 00:32:47,040
from the file into memory. These are some flags and protection. The other very important fields
从文件加载到内存中。这些是一些标志和保护。另外还有一些非常重要的字段。

223
00:32:47,040 --> 00:32:54,800
are the address and the length. The length is how much from the file you want to map in memory.
地址和长度是指定的内容。长度表示你想要在内存中映射的文件大小。

224
00:32:54,800 --> 00:33:01,120
And the address, you can even give an address. And if you give an address, then the file system
和地址，你甚至可以给一个地址。如果你给一个地址，那么文件系统就会

225
00:33:01,120 --> 00:33:11,040
is going to try to map the file at the address it is given. But if it cannot, then it may find
将尝试在给定的地址上映射文件。但如果无法映射，则可能会找到

226
00:33:11,040 --> 00:33:18,640
another place in the physical memory with enough space to map the file, which basically is a
另一个物理内存中有足够空间来映射该文件的位置，基本上是一个

227
00:33:18,640 --> 00:33:28,880
contiguous region of length length. And in either way, the function returns the address in the
连续长度为length的区域。无论哪种方式，该函数都会返回该地址。

228
00:33:28,880 --> 00:33:42,000
memory where you map the file. OK. In most of the cases, you can use also this memory map
内存中你映射文件的位置。好的。在大多数情况下，你也可以使用这个内存映射。

229
00:33:42,000 --> 00:33:46,400
also to communicate between processes. We'll talk briefly about that later.
还可以用来在进程之间进行通信。稍后我们会简要讨论这个。

230
00:33:46,400 --> 00:33:55,840
But here is an example of a program to just give you a sense of what memory map files
但是这里有一个程序示例，只是为了让你对内存映射文件有个概念。

231
00:33:56,400 --> 00:34:07,680
means and what is there in effect. So here is a simple program. And just for the sake of it,
意思是什么以及实际上有什么效果。所以这里有一个简单的程序。就为了好玩，

232
00:34:07,680 --> 00:34:14,480
we are going to type three addresses to bring three addresses. One is something, is the address
我们将要输入三个地址，以获取三个地址。其中一个是something，是地址。

233
00:34:14,480 --> 00:34:23,440
of something, this variable something. And that something is when it's a data, right?
关于某事，这个变量是某事。而那个某事是指数据，对吗？

234
00:34:24,240 --> 00:34:32,240
It's a data segment, right? Then we are going to allocate something. And everything which is
这是一个数据段，对吗？那么我们要分配一些东西。而且所有的东西，无论是什么类型的数据，都将被存储在这个数据段中。

235
00:34:32,240 --> 00:34:40,800
allocated, you know it's on the heap. So this is address for the result of MLK. It resides in the
分配的，你知道它在堆上。所以这是MLK结果的地址。它驻留在

236
00:34:40,800 --> 00:34:48,880
heap. And then the last one is you have this my file here variable, which is a local variable.
堆。然后最后一个是你有这个my file here变量，它是一个局部变量。

237
00:34:48,880 --> 00:34:55,200
And we know that the local variables of the functions are put on the stack. So this address
而且我们知道函数的局部变量被放在栈上。所以这个地址

238
00:34:55,200 --> 00:35:02,640
refers on-- it's an address in the stack segment. Then you open a file. And this is a file we want
refers on-- 它是堆栈段中的一个地址。然后你打开一个文件。而这个文件是我们想要的。

239
00:35:02,640 --> 00:35:12,240
to map in the memory. And now we map the file in the memory. So basically, we are taking this file
将文件映射到内存中。现在我们将文件映射到内存中。基本上，我们正在处理这个文件。

240
00:35:13,920 --> 00:35:20,800
descriptor. We are going to pass to a map the file descriptor my_fd. And zero, meaning that I want to
将文件描述符my_fd传递给一个映射。而零表示我想要...

241
00:35:20,800 --> 00:35:28,240
start mapping from the first byte in the file. And here there are again some flags about read,
从文件的第一个字节开始进行映射。这里再次提到了一些关于读取的标志。

242
00:35:28,240 --> 00:35:33,120
write. And this is about whether this can be shared between different processes.
写。这是关于这个是否可以在不同的进程之间共享的问题。

243
00:35:33,120 --> 00:35:41,120
And the other important thing is that I am the first address where to map
而另一个重要的事情是，我是首选的地图绘制地址。

244
00:35:42,080 --> 00:35:51,840
this program passes as being zero. This time, the operating system, you are free to choose a region
这个程序通过为零。这次，操作系统，你可以自由选择一个地区。

245
00:35:51,840 --> 00:35:58,480
of memory in the memory where to map this file. And most of the cases, this is what you are going
将该文件映射到内存中的内存位置。在大多数情况下，这就是你需要的。

246
00:35:58,480 --> 00:36:04,960
to give. This argument will be zero, right? Because you don't know better than the operating system
给。这个争论会是零，对吗？因为你不比操作系统更懂。

247
00:36:04,960 --> 00:36:13,920
where you should put this file in memory. And my file is a return the starting address
你应该将这个文件放在内存的哪个位置。而我的文件是返回起始地址的。

248
00:36:13,920 --> 00:36:19,280
allocated for that file to be mapped, where the file is mapped to.
分配给该文件进行映射的内存空间，文件被映射到哪里。

249
00:36:19,280 --> 00:36:33,440
So now, and then you print the file, right? So you pass the file. This is an argument of this
所以现在，然后你打印文件，对吗？所以你传递文件。这是这个参数的一个例子。

250
00:36:34,080 --> 00:36:44,320
program and you print it. And then you write something at the address of the file in memory
程序，你打印它。然后你在内存中的文件地址处写入一些内容。

251
00:36:44,320 --> 00:36:52,000
plus 10. Let's write over it, okay? So let's see what happens. So first of all, when I print it,
加10。我们来覆盖它，好吗？那么我们来看看会发生什么。首先，当我打印它的时候，

252
00:36:52,000 --> 00:36:57,440
this is when you print the file. Let's say this is a question. This is the content of the file,
这是你打印文件的时候。假设这是一个问题。这是文件的内容。

253
00:36:58,080 --> 00:37:05,680
right? So it's mapped. So the first three lines, so I'm invoking this program with argument test.
对吗？所以它已经映射了。所以前三行，我是用参数“test”来调用这个程序的。

254
00:37:05,680 --> 00:37:13,840
The first three lines are the print-ups I talk about, printing addresses, the data segment,
第一行是我所说的打印输出，打印地址，数据段，

255
00:37:13,840 --> 00:37:26,720
heap or stack. And then the next five lines are written by these print-ups. And it's basically
堆或栈。然后下面的五行是由这些打印输出写的。基本上，

256
00:37:26,720 --> 00:37:32,240
the content of the test file. So the content of the test files, there are four lines. This is line
测试文件的内容。所以测试文件的内容，有四行。这是第一行。

257
00:37:32,240 --> 00:37:40,080
one, line two, line three, and line four. And now here, I'm now this file, every content,
一，第二行，第三行和第四行。现在在这里，我现在这个文件，每个内容，

258
00:37:40,080 --> 00:37:45,040
the content is mapped in memory. And now in memory, like I mentioned, I'm writing at the
内容被映射到内存中。现在在内存中，就像我之前提到的，我正在写入。

259
00:37:45,040 --> 00:37:54,320
address plus 20. Let's write over it. And then I close the file. And then I want to see what is in
地址加20。让我们在上面写。然后我关闭文件。然后我想看看里面有什么。

260
00:37:54,320 --> 00:38:01,600
the file after I close it. And I do cut.test. And this is what I find. This is line one, okay?
我关闭文件后的文件。我进行了剪切测试。这是我发现的。这是第一行，好吗？

261
00:38:01,600 --> 00:38:09,360
And then this, and then you can see, let's write over it. Because this means that, obviously,
然后这样，然后你就可以看到，我们在上面写。因为这意味着，显然，

262
00:38:09,360 --> 00:38:23,680
I wrote, starting from the character 21st, the 21st character in the file, I wrote the string,
我从第21个字符开始，文件中的第21个字符，我写下了这个字符串。

263
00:38:23,680 --> 00:38:30,160
let's write over, which is exactly, and then I close the file. And obviously, everything was saved
让我们覆盖写入，这样就完全一样了，然后我关闭文件。显然，一切都已保存。

264
00:38:30,160 --> 00:38:39,040
and including the changes in this file. Does it make sense?
并包括这个文件中的更改。这样说通吗？

265
00:38:39,040 --> 00:38:46,800
It's pretty cool. You should try it. Okay.
很酷的。你应该试试看。好的。

266
00:38:51,520 --> 00:38:59,200
So, it's like I mentioned, you can share the files between two different processes.
所以，就像我提到的那样，你可以在两个不同的进程之间共享文件。

267
00:38:59,200 --> 00:39:04,640
So here, I have the same file and I have two virtual addresses from two processes.
所以在这里，我有相同的文件，并且我有来自两个进程的两个虚拟地址。

268
00:39:04,640 --> 00:39:12,320
And you can share, you can map the file in memory and can be shared this file
而且你可以共享，你可以将文件映射到内存中，并且可以共享这个文件。

269
00:39:12,320 --> 00:39:18,720
between these two processes. It's exactly like a real file, the file on the disk can be
在这两个过程之间。它就像一个真实的文件一样，磁盘上的文件可以

270
00:39:18,720 --> 00:39:32,480
shared between different processes. Also, you can use this map to share the data,
共享在不同进程之间。此外，您可以使用这个映射来共享数据，

271
00:39:32,480 --> 00:39:39,120
or to share memory and use the memory to share the data between the parent and the children
或者共享内存并使用内存在父进程和子进程之间共享数据。

272
00:39:39,120 --> 00:39:44,560
and your four children. You can also create an mf file, it's anonymous file, because there is not
和你的四个孩子。你也可以创建一个mf文件，它是匿名文件，因为没有

273
00:39:44,560 --> 00:39:51,280
a real file. And you can share to the children and then you can use it to pass data between the
一个真实的文件。你可以分享给孩子们，然后可以用它来在之间传递数据。

274
00:39:51,280 --> 00:39:58,400
children and the parents with this anonymous memory mf file. Of course, this is not backed
孩子和父母使用这个匿名的记忆mf文件。当然，这并没有得到支持。

275
00:39:58,400 --> 00:40:00,480
by the disk. Okay.
在磁盘上。好的。

276
00:40:10,720 --> 00:40:17,120
Yeah, the question here, is a file printed by the printf or the
是的，这里的问题是，文件是由printf函数打印的还是由其他函数打印的？

277
00:40:17,120 --> 00:40:26,880
foots? So here it's printed by the printf, this file. Puts is a special case
foots? 这里是由printf打印的，这个文件。puts是一个特殊情况。

278
00:40:26,880 --> 00:40:38,720
of printf. With printf, it's formatting, you can print different values in the same instruction.
使用printf函数，通过格式化，可以在同一条指令中打印不同的值。

279
00:40:38,720 --> 00:40:43,280
So here, you can see the first argument you are going to print.
所以在这里，你可以看到你要打印的第一个参数。

280
00:40:43,280 --> 00:40:53,760
Because you are printing the map address and the second argument, you are
因为你正在打印地图地址和第二个参数，所以你需要

281
00:40:53,760 --> 00:41:02,080
printing what? You are printing what is the address of m file. And there what you have,
打印什么？你正在打印的是m文件的地址。然后你有什么？

282
00:41:02,880 --> 00:41:11,440
you know, a bunch of, you basically have a bunch of characters, you have a string,
你知道，一堆，基本上你有一堆字符，你有一个字符串。

283
00:41:11,440 --> 00:41:15,600
right? And you bring everything what you find there.
对吗？你把那里的一切都带回来。

284
00:41:15,600 --> 00:41:19,520
Okay.
Sure. Please go ahead and provide me with the text you would like me to translate.

285
00:41:19,520 --> 00:41:26,560
So,
所以，

286
00:41:26,560 --> 00:41:41,440
so now spans the project three design document, remember is due Saturday, this Saturday, and the
所以现在项目涵盖了三份设计文件，记住截止日期是星期六，就是这个星期六。

287
00:41:41,440 --> 00:41:55,680
homework five is due on Monday. Basically, almost two weeks from now. So any questions about memory
作业五要在周一之前完成。基本上，就是从现在算起差不多两周的时间。所以关于记忆方面有什么问题吗？

288
00:41:55,680 --> 00:42:08,720
map files? Okay, the next thing we are going to go over is a buffer cache. So remember, we discussed
地图文件？好的，接下来我们要讲的是缓冲区缓存。所以记住，我们之前讨论过的是

289
00:42:08,720 --> 00:42:14,880
that when you read from a file, you will create different copies. And one of these copies is in
内存 (memory).

290
00:42:14,880 --> 00:42:20,160
the buffer cache, it's in the operating system can maintain a cache of the data from the file.
缓冲区高速缓存，它位于操作系统中，可以维护文件数据的缓存。

291
00:42:20,800 --> 00:42:28,720
And in this case, you can put the data, you can put inodes, directory, directories, and so forth.
在这种情况下，你可以放置数据、放置索引节点、目录、目录等等。

292
00:42:28,720 --> 00:42:35,280
And some of these pages which are cached can be dirty, obviously. And then the operating
一些被缓存的页面显然可能是脏的。然后操作系统

293
00:42:35,280 --> 00:42:40,400
system has to take care that to write back eventually the dirty pages to the disk.
系统必须确保最终将脏页写回磁盘。

294
00:42:44,960 --> 00:42:52,640
And really what you want the key idea, why do you want to cache the disk data into memory again,
而实际上，你想要的关键点是什么，为什么你想要将磁盘数据再次缓存到内存中呢？

295
00:42:52,640 --> 00:42:59,280
this is an operating system memory, right? It's not in the user space memory, like memory map files,
这是操作系统的内存，对吗？它不是用户空间的内存，比如内存映射文件，

296
00:42:59,280 --> 00:43:04,960
you copy directly in the user space for this is in the operating system. So to get this data,
你直接在用户空间中复制，这是在操作系统中进行的。因此，要获取这些数据，

297
00:43:04,960 --> 00:43:09,360
even if it's in memory, it's in the memory of the operating system, you still need to make a
即使它在内存中，也是在操作系统的内存中，你仍然需要进行一些操作。

298
00:43:10,880 --> 00:43:18,480
system call to get that data. And obviously you cache it to exploit the locality,
系统调用以获取这些数据。显然，你要将其缓存以利用局部性。

299
00:43:18,480 --> 00:43:27,760
right? There are many accesses to the same blocks. So if you have them in memory, it's much faster
对吗？同一个块有很多种访问方式。所以如果你将它们存储在内存中，速度会快得多。

300
00:43:27,760 --> 00:43:34,240
than accessing the disk. So you need to do this translation between a pass,
比访问磁盘更快。所以你需要在一次通行中完成这个翻译。

301
00:43:35,040 --> 00:43:41,280
name of the file and the inodes, and then from the block address to the disk contact, right?
文件的名称和索引节点，然后从块地址到磁盘联系，对吗？

302
00:43:41,280 --> 00:43:51,760
The logical block addresses. So let's see how the buffer cache is working. And here it's setting up.
逻辑块地址。那么让我们看看缓冲区缓存是如何工作的。这里正在进行设置。

303
00:43:51,760 --> 00:43:59,840
So this is on the right hand side, you have the disk. This is like FFS layout on the disk.
所以这是在右手边，你有一个磁盘。这就像磁盘上的FFS布局。

304
00:44:00,960 --> 00:44:05,520
And now this is a buffer cache and the buffer cache has a bunch of blocks.
现在这是一个缓冲区缓存，缓冲区缓存中有一堆块。

305
00:44:05,520 --> 00:44:15,200
And the blocks typically are the same size as pages, memory pages. And then you are going to
而这些块通常与页面大小相同，即内存页面。然后你要做的是什么？

306
00:44:15,200 --> 00:44:23,120
cache the blocks and you can have data blocks, like I mentioned, they can be inodes and they can
缓存这些块，你就可以拥有数据块，就像我之前提到的那样，它们可以是索引节点。

307
00:44:23,120 --> 00:44:32,400
be directory blocks. In Avishon, you have a free, also you can have the free bitmap. Remember,
是目录块。在Avishon中，你有一个空闲的，也可以有空闲位图。记住，

308
00:44:32,400 --> 00:44:42,240
you do have the free bitmap on the disk to maintain data about what are the available
你确实在磁盘上有一个免费的位图来维护关于可用的数据的信息。

309
00:44:42,240 --> 00:44:50,560
blocks, free blocks. Okay. So this is what you have. These blocks are
块，空闲块。好的。这是你拥有的东西。这些块是

310
00:44:51,280 --> 00:44:59,760
the cache is organized as an array of blocks and the color represent different kinds of types of
缓存被组织成一个块的数组，颜色代表不同类型的种类。

311
00:44:59,760 --> 00:45:06,720
information. And here you have a state, whether in this case is free or dirty or
信息。在这里，你有一个状态，无论在这种情况下是自由的还是肮脏的或者

312
00:45:06,720 --> 00:45:10,480
things like that, or what happens, whether you read or write a block.
这样的事情，或者发生了什么，无论你是读还是写一个块。

313
00:45:10,480 --> 00:45:19,360
So let's see what happens. So let's say I am reading an inode, sorry, I am reading a data block.
那么让我们看看会发生什么。假设我正在读取一个inode，抱歉，我是在读取一个数据块。

314
00:45:20,080 --> 00:45:22,880
So I am reading a data block, a new data block from the disk.
所以我正在读取一个数据块，一个来自磁盘的新数据块。

315
00:45:22,880 --> 00:45:28,320
I am going to find an available block in the cache.
我将要在缓存中找到一个可用的块。

316
00:45:28,320 --> 00:45:36,720
And then I am going to write, now you see the state is read. This means that now I am
然后我要写，现在你看到的状态是红色的。这意味着现在我正在

317
00:45:36,720 --> 00:45:44,240
reading the data from the block on the disk into this available space. Right?
从磁盘上的块中读取数据到这个可用空间中。对吗？

318
00:45:47,280 --> 00:45:58,000
And then once you read it, you are going to mark that it's no longer free. And if you write it,
然后一旦你阅读了它，你会标记它不再免费。如果你写下来，

319
00:45:58,000 --> 00:46:04,560
you have to mark it that it's dirty because now this is modified and it's no longer the
你必须标记它为脏的，因为现在它已经被修改，不再是原来的了。

320
00:46:04,560 --> 00:46:16,560
same content as a page on the disk. So now you say you are going to read a
与磁盘上的页面内容相同。所以现在你说你要读取一个什么？

321
00:46:16,560 --> 00:46:23,520
directory. Right? It's the same thing. You are okay if you look, you find an available block and
目录。对吗？其实是一回事。如果你查找，找到一个可用的块，那就没问题了。

322
00:46:23,520 --> 00:46:28,000
you read it there. Right? And you say what it is, inode.
你在那里读到了。对吗？你说它是什么，inode。

323
00:46:28,000 --> 00:46:44,480
Okay. And this is if you do it, the data is the same thing. Right? If you write it,
好的。这是如果你这样做的话，数据就是同样的东西。对吗？如果你写下来的话，

324
00:46:44,480 --> 00:46:50,000
now you say dirty, the data is dirty. This is what if you write in the block.
现在你说脏话了，数据也是脏的。这是你在区块中写的内容。

325
00:46:50,000 --> 00:46:59,600
Right? If you write to a block, you first, if the block is not in the cache, you bring the block in
对吗？如果你要写入一个块，首先，如果该块不在缓存中，你需要将该块加载进来。

326
00:46:59,600 --> 00:47:06,160
the cache and you write in the block on the cache in memory and you mark it as dirty.
缓存和你在缓存中写入的块，你将其标记为脏块。

327
00:47:06,160 --> 00:47:09,600
Any questions?
有任何问题吗？

328
00:47:13,120 --> 00:47:18,800
So discussion. So the buffer cache is entirely implemented in the operating system.
所以讨论一下。所以缓冲区高速缓存完全是在操作系统中实现的。

329
00:47:18,800 --> 00:47:29,840
Right? It's in the software. There is now TLB. Okay. And the blocks go through transitional
对吗？这是在软件中。现在有TLB。好的。然后这些块经历过渡。

330
00:47:29,840 --> 00:47:35,360
state between free and use, being read from the disk, being written to the disk and so forth.
状态介于空闲和使用之间，从磁盘中读取，写入磁盘等等。

331
00:47:37,280 --> 00:47:43,600
And blocks contain all this variable, you know, variety of information like I mentioned,
并且这些块包含了所有这些变量，你知道的，像我之前提到的那样，各种各样的信息。

332
00:47:43,600 --> 00:47:47,280
inodes and data and directories and the freemap.
i节点、数据、目录和空闲位图。

333
00:47:47,280 --> 00:47:55,360
And the OS is going to maintain the management, to maintain the pointer into them.
并且操作系统将维护管理，维护指针指向它们。

334
00:47:55,360 --> 00:48:05,520
It's also the OS is going to manage this buffer cache when the process exits, when writes reads.
这个操作系统也会在进程退出时、写入和读取时管理这个缓冲区缓存。

335
00:48:06,240 --> 00:48:13,360
And for instance, on the process exits, you are going to flush to write back all the modified
例如，当进程退出时，你将会刷新以写回所有修改过的内容。

336
00:48:13,360 --> 00:48:17,920
data from the buffer cache to the disk.
将缓冲区的数据写入磁盘。

337
00:48:17,920 --> 00:48:25,600
Now, this is a cache, it can fill up. So what happens when you fill up? Well, there is a
现在，这是一个缓存，它可以填满。那么当你填满时会发生什么呢？嗯，有一个情况。

338
00:48:25,600 --> 00:48:32,000
replacement policy. Like we know, like demand paging, if you remember. Replacement policy,
替换策略。就像我们所知道的，就像需求分页一样，如果你还记得的话。替换策略，

339
00:48:32,000 --> 00:48:36,480
what can you do? You know, you can do allerio. It worked pretty well for demand paging. So I
我可以做什么？你知道，你可以做allerio。它在需求分页中表现得非常好。所以我可以做的是：

340
00:48:36,480 --> 00:48:42,480
want to do something like this. And this works well. And if the memory is big enough, especially
想要做类似的事情。这个方法很有效。如果内存足够大，尤其是...

341
00:48:42,480 --> 00:48:47,680
it can accommodate the working set of the particular
它可以容纳特定的工作集。

342
00:48:47,680 --> 00:48:54,960
applications, the working set in terms of the data which is accessed from the file.
应用程序中的工作集是指从文件中访问的数据。

343
00:48:59,040 --> 00:49:06,720
Now, the disadvantage is that it's for some access patterns, allerio is not good.
现在，缺点是对于某些访问模式，allerio并不好。

344
00:49:06,720 --> 00:49:13,920
So for instance, if I'm just going to scan the file, I'm just going to read
所以例如，如果我只是要扫描这个文件，我只需要读取它。

345
00:49:13,920 --> 00:49:17,280
and to read to need, I'm going to read a block only once.
并且为了阅读需要，我打算只读一次一个块。

346
00:49:17,280 --> 00:49:26,720
So with allerio, that block is going to displace another block in the buffer cache.
所以使用allerio，该块将会将缓冲区缓存中的另一个块替换掉。

347
00:49:26,720 --> 00:49:33,840
But then the block was loaded into the cache. I am no longer to access again,
但是这个数据块已经被加载到缓存中了。我无法再次访问它了。

348
00:49:33,840 --> 00:49:39,920
because I got it later. And I'm just doing a scan. So that's a problem.
因为我后来才明白。而且我只是在进行一次扫描。所以这是个问题。

349
00:49:39,920 --> 00:49:47,200
So what is the solution here? It's a smart solution. The solution here is that the operating
所以这里的解决方案是什么？这是一个聪明的解决方案。这里的解决方案是操作的。

350
00:49:47,200 --> 00:49:55,360
system allows applications to request other policies. So the application knows more about
系统允许应用程序请求其他策略。这样应用程序就能更多地了解。

351
00:49:55,360 --> 00:50:01,360
what is the access pattern. So if the application knows that it's only going to scan the file,
访问模式是什么？所以，如果应用程序知道它只会扫描文件，

352
00:50:01,360 --> 00:50:10,800
it can specify as a replacement policy used once. And this means that the file system can discard
它可以被指定为一种一次性使用的替换策略。这意味着文件系统可以丢弃该文件。

353
00:50:10,800 --> 00:50:16,720
the block as soon as they are used. They don't need to operating system doesn't need to maintain
一旦使用，这些块就会被锁定。操作系统不需要维护。

354
00:50:16,720 --> 00:50:21,040
that block in the buffer cache.
在缓冲区缓存中的那个块。

355
00:50:21,040 --> 00:50:34,640
Now, the cache is obviously stored in the memory. So one question is that how much memory
现在，缓存显然存储在内存中。所以一个问题是，有多少内存。

356
00:50:34,640 --> 00:50:43,680
you should give to the operating system for the buffer cache versus the memory for the map,
你应该将缓冲区缓存的内存与地图的内存分配给操作系统。

357
00:50:43,680 --> 00:50:50,240
the processes virtual address space. If you give too much memory to the file system cache,
进程的虚拟地址空间。如果你给文件系统缓存分配太多内存，

358
00:50:50,240 --> 00:50:57,120
the access can be fast, but you may not have enough room left for the applications.
访问速度可能很快，但您可能没有足够的空间来安装应用程序。

359
00:50:57,120 --> 00:51:02,640
If you give too little memory, you are going to have a lot of accesses to the disk,
如果你分配的内存太少，你将会频繁访问硬盘，

360
00:51:02,640 --> 00:51:09,120
because you cannot cache the blocks which are going to be used frequently. So the solution here
是因为你无法缓存那些经常使用的块。所以这里的解决方案是

361
00:51:09,120 --> 00:51:18,880
is the operating system adjusts dynamically the boundary between the OS cache and the
操作系统是否动态调整操作系统缓存和应用程序之间的边界？

362
00:51:18,880 --> 00:51:25,920
application memory so that the amount of the rate of paging in are roughly the same.
应用程序内存，以使分页率的数量大致相同。

363
00:51:25,920 --> 00:51:30,480
Okay. Any questions so far?
好的。到目前为止有任何问题吗？

364
00:51:44,000 --> 00:51:49,760
Okay. Another thing about the file systems you see a lot is prefetching.
好的。关于文件系统，还有一个常见的功能是预读取。

365
00:51:49,760 --> 00:51:55,760
Prefetching you remember is read ahead. And we discussed that also in the case of demand paging.
预取是指提前读取。我们还讨论了在需求分页的情况下也是如此。

366
00:51:55,760 --> 00:52:03,920
Demand paging is to avoid compulsory misses. If the access is sequential, you know that if you

367
00:52:03,920 --> 00:52:13,200
 have one, if you access one block, then you are going to very likely read the data from the next

368
00:52:13,200 --> 00:52:22,240
 block. So why not bring both blocks right away in memory, right? And avoid to have another access to

369
00:52:22,240 --> 00:52:33,440
 the disk, which is very expensive. Right. Of course, you have a question here, how many,

370
00:52:33,440 --> 00:52:39,760
 you know, how much you are going to read ahead? How many blocks are you going to prefetch?

371
00:52:41,280 --> 00:52:48,320
 If you do too many, then it's fine, but you are going to be wasteful. Right.

372
00:52:48,320 --> 00:52:57,200
 And if you are going to do too little, then you are going to have a lot of more accesses.

373
00:52:57,200 --> 00:53:06,800
 So notice two things though. So one is prefetching is easy if the blocks are

374
00:53:07,760 --> 00:53:14,240
 sequentially arranged on the disk. Right. Because it's one sick time and you just

375
00:53:14,240 --> 00:53:20,160
 read multiple blocks. Right. The other thing is

376
00:53:20,160 --> 00:53:31,680
 if you are going, even if the blocks are not sequential, then you are going to provide

377
00:53:32,320 --> 00:53:40,000
 the disk controller a bunch of blocks you want to read. And you remember the elevator algorithm

378
00:53:40,000 --> 00:53:49,440
 two lectures ago? The elevator algorithms reorder the request, if it gets more requests at the same

379
00:53:49,440 --> 00:53:58,000
 time, so that it uses the sick time. It puts them in the order along the direction that the head is

380
00:53:58,000 --> 00:54:06,320
 moving. Excuse me. So the head doesn't go back and forth. It's just going in one direction and is

381
00:54:06,320 --> 00:54:11,840
 going to get small sick times to the next request, minimizing the sick time.

382
00:54:11,840 --> 00:54:19,600
 So the more prefetching you do, the more requests you are going to give the disk,

383
00:54:19,600 --> 00:54:26,800
 the more potential for the elevator algorithms to be efficient and to minimize the sick time.

384
00:54:26,800 --> 00:54:36,480
 So how much to prefetch? You just discussed. Too much. It's going to be wasteful,

385
00:54:36,480 --> 00:54:41,360
 both because delays, because it takes maybe a little bit more time to prefetch multiple blocks,

386
00:54:41,360 --> 00:54:50,560
 but also because you are going to bring this data in the buffer cache. And maybe you are not going

387
00:54:50,560 --> 00:54:56,640
 to use it. And this data, the blocks you are not going to use it, maybe replaced some blocks you

388
00:54:56,640 --> 00:55:03,280
 are going, which are used frequently. And too little prefetching, many more sick. If you have

389
00:55:03,280 --> 00:55:09,040
 the limit, you prefetch only one, you prefetch nothing. And then you are going to have a sick

390
00:55:09,040 --> 00:55:18,240
 for each new block you are accessing. This was about reading, reading multiple blocks.

391
00:55:19,760 --> 00:55:28,480
 The same you can also about writes. It's called delayed writes. So delay writes basically means

392
00:55:28,480 --> 00:55:36,560
 that it's again, when you write the data and you modify the data, the data is modified only in

393
00:55:36,560 --> 00:55:47,520
 memory in the buffer cache. So it is not written immediately on the disk. So therefore the writes

394
00:55:47,520 --> 00:55:53,360
 are very fast, they are on to memory. The writes to the file are writes to memory, if the block is

395
00:55:53,360 --> 00:56:01,280
 in the buffer cache. The read is going to first go to the buffer cache to see whether the block

396
00:56:01,280 --> 00:56:07,440
 they want to read is in the buffer cache. And if it is in, it's going to get the most up-to-date

397
00:56:07,440 --> 00:56:16,480
 value. Even if that block was not written yet on the disk, it's still the writes, previous write

398
00:56:16,480 --> 00:56:27,040
 updated the block in the memory. So we are fine here. So when does the write, the changes made

399
00:56:27,040 --> 00:56:35,440
 by a write reach the disk? There are a few cases. One is the operating system is flashing it

400
00:56:35,440 --> 00:56:45,440
 periodically. I think like say 30 seconds. Every 30 seconds, you go and scan the buffer cache and

401
00:56:45,440 --> 00:56:56,080
 you look at the blocks which are dirty and write them on the disk. Or obviously, when the cache is

402
00:56:56,080 --> 00:57:04,720
 full, then you may want to flash them to write on the disk. So what are the delay writes advantages?

403
00:57:04,720 --> 00:57:09,040
 Why don't you tell me here? What do you think are the delayed write advantages?

404
00:57:13,040 --> 00:57:26,720
 Compared with you write back every time you modify a block.

405
00:57:26,720 --> 00:57:36,880
 Yes, you do not have to go all the way to the disk for changing every time.

406
00:57:38,080 --> 00:57:43,760
 That's very good. Can you think about anything else?

407
00:57:43,760 --> 00:57:55,920
 So one is, again, if you don't need to go to the disk, you are going to have a performance advantage

408
00:57:55,920 --> 00:57:59,840
 obviously, because you're like I said, you just write to memory, you don't write to the disk.

409
00:58:00,560 --> 00:58:11,040
 It's also you can accumulate multiple writes. You can write to the same block multiple times

410
00:58:11,040 --> 00:58:22,000
 before you are going to write the block to the disk. So you have only one write to the disk

411
00:58:22,000 --> 00:58:29,040
 corresponding to many updates of that block. The other thing is for the reads.

412
00:58:30,000 --> 00:58:35,680
 If you are going to write multiple pages to the disk at the same time, you are going to

413
00:58:35,680 --> 00:58:42,160
 send all these requests to the disk. And the disk, the elevator algorithms on the disk,

414
00:58:42,160 --> 00:58:49,280
 again reorder these requests so that they are going to write close together

415
00:58:49,280 --> 00:58:54,400
 blocks one after another and again minimize the sigma.

416
00:58:54,400 --> 00:59:07,680
 Another thing is that for the layerized, you also may want to allocate multiple blocks at the same

417
00:59:07,680 --> 00:59:19,200
 time for a file to keep them contiguous. You can make room when you write. So you write multiple

418
00:59:19,200 --> 00:59:29,840
 contiguous files. So what I'm trying to say here is that when you are going to write

419
00:59:29,840 --> 00:59:37,120
 data and I have multiple blocks who are modified from or the new blocks, because you can create

420
00:59:37,120 --> 00:59:42,000
 new blocks which are in memory, they are not on the disk, if you expand the file. If you

421
00:59:42,000 --> 00:59:47,200
 write at the end of the file, the file on the disk is smaller. But now you are going to write,

422
00:59:47,200 --> 00:59:52,640
 you are going to allocate more data in the buffer cache. So now the file in memory is larger

423
00:59:52,640 --> 01:00:01,200
 than the file on the disk. So now if I wrote, say, worth of 10 blocks in memory, then if I delay to

424
01:00:01,200 --> 01:00:06,960
 write these 10 blocks to the disk, when I write them, the disk now are these 10 blocks and I'm

425
01:00:06,960 --> 01:00:13,440
 telling them from the same file. So maybe you can find a contiguous place on the disk to place

426
01:00:13,440 --> 01:00:20,720
 all these 10 blocks, one after another, which again is convenient when you read the data from

427
01:00:20,720 --> 01:00:26,080
 these blocks because it's there, you know, one after another. So it's contiguous region of the disk.

428
01:00:26,080 --> 01:00:33,200
 Again, you avoid in that case, we are going to avoid the seek and the rotation like this.

429
01:00:33,200 --> 01:00:43,280
 And here is a fun thing. If you have some files, you open a file during your application,

430
01:00:44,080 --> 01:00:52,160
 and you close it, you just use from some to have a file to create a file to share some data

431
01:00:52,160 --> 01:00:58,320
 between two applications. If that file is very short duration, it may never make on the disk,

432
01:00:58,320 --> 01:01:06,640
 right? Because it's going to be removed and it's going to be deleted, closed, right? And deleted

433
01:01:06,640 --> 01:01:17,520
 before you have the operating system have a chance to write it on the disk. Okay, so here I have a

434
01:01:17,520 --> 01:01:28,400
 question for you. So with demand paging, remember with demand paging, we have an ALERU and the same

435
01:01:28,400 --> 01:01:35,360
 kind of algorithms, but we are very paranoid then about the performance of the ALERU. And we

436
01:01:35,360 --> 01:01:43,360
 implemented some approximations and like, you know, clock algorithms and things like that,

437
01:01:43,360 --> 01:01:50,880
 second chance algorithm. With buffer caching, it turns out that people do not implement exact ALERU

438
01:01:50,880 --> 01:02:00,480
 in the operating system. So people are not as concerned about the performance of ALERU caching

439
01:02:01,280 --> 01:02:08,640
 for buffer caching versus demand paging. Why do you think is that?

440
01:02:08,640 --> 01:02:25,040
 So why do you think that in the case of the buffer caching is fine to implement the exact ALERU,

441
01:02:25,600 --> 01:02:31,440
 while the context of demand paging is not and we need to implement approximation?

442
01:02:31,440 --> 01:02:43,600
 It's in software, so not that much of a performance hit.

443
01:02:43,600 --> 01:02:50,560
 Well, paging is also in software, so demand paging is implemented in software as well.

444
01:02:50,560 --> 01:03:04,400
 Anyone?

445
01:03:04,400 --> 01:03:14,720
 I think it's a subtle question and it's okay, you know, if you do not

446
01:03:17,840 --> 01:03:25,040
 know the answer to this question. So one reason is about, think about the following thing.

447
01:03:25,040 --> 01:03:31,440
 Demand paging, the expectation of the application is going to, you are going to read and write from

448
01:03:31,440 --> 01:03:40,240
 memory. So this is what you are against. You are against in the latency provided by memory because

449
01:03:40,240 --> 01:03:48,480
 that's the expectation of the application, it accesses memory. So you need to be very fast.

450
01:03:48,480 --> 01:03:56,720
 In contrast, when you access a file, you don't have a, you know, expectation to be that fast.

451
01:03:56,720 --> 01:04:05,440
 Buffer cache is improving the latency, but the baseline is accessing the disk, which is slow.

452
01:04:05,440 --> 01:04:10,240
 And sometimes, yeah, you have to access the disk because it's a buffer, you know, that

453
01:04:10,240 --> 01:04:18,880
 the data is not in memory and so forth. So, you know, so that's kind of one reason,

454
01:04:18,880 --> 01:04:26,960
 right? Because the expectation when you access a disk is not to have the fastest

455
01:04:26,960 --> 01:04:34,080
 operations, right? So therefore it's okay if you are, you know, if you are a little bit slower

456
01:04:34,720 --> 01:04:40,880
 and actually with being a little bit slower, you can manage the cache even better, right?

457
01:04:40,880 --> 01:04:43,680
 Because it's no longer an approximation algorithm.

458
01:04:43,680 --> 01:04:54,720
 Eviction policy, demand paging.

459
01:04:54,720 --> 01:05:03,120
 Remember, you have big non-recently used pages when memory is slow, close to full.

460
01:05:04,080 --> 01:05:10,720
 With buffer cache is different. You write back dirty blocks periodically,

461
01:05:10,720 --> 01:05:19,840
 even if used recently, right? And why is that? The reason for that is again, different expectations.

462
01:05:19,840 --> 01:05:25,840
 When I read and write to memory, I don't have any expectation about the ability of the data,

463
01:05:27,040 --> 01:05:33,920
 right? You do expect that the computer crashes or even the program crashes.

464
01:05:33,920 --> 01:05:40,960
 You lose the data in memory, right? But if you write on the disk, if you think that you write

465
01:05:40,960 --> 01:05:49,280
 in a file, you do assume that what you wrote is persistent, right?

466
01:05:50,720 --> 01:05:57,680
 So now, and we'll talk about this quite a bit for the remaining of this lecture and the next lecture.

467
01:05:57,680 --> 01:06:06,960
 You need the operating system to try pretty hard to provide that persistence. So one way

468
01:06:06,960 --> 01:06:16,400
 is going it is by periodically writing the updated modified blocks from the buffer cache to the disk.

469
01:06:16,400 --> 01:06:33,920
 Yes, the marginal difference between ALERIO and non-ALERIO isn't that big in this context,

470
01:06:33,920 --> 01:06:40,880
 since this baseline, since the baseline is a disk operation which takes super long anyways.

471
01:06:41,680 --> 01:06:48,640
 So this is a comment by Gilbert and yeah, it's exactly correct, right? So in the case of the

472
01:06:48,640 --> 01:06:55,440
 buffer cache, the baseline is a disk. In the case of demand paging, the baseline is a memory.

473
01:07:03,120 --> 01:07:14,560
 Okay, so it's like we mentioned, in order to improve the persistency,

474
01:07:14,560 --> 01:07:21,360
 the operating system writes data to the disk from the buffer cache periodically.

475
01:07:21,360 --> 01:07:26,080
 So it flashes it like in the case of Linux every 30 seconds.

476
01:07:28,640 --> 01:07:34,480
 However, obviously this is not bulletproof, right?

477
01:07:34,480 --> 01:07:42,880
 What if the operating system or you have a failure, the machine fails

478
01:07:42,880 --> 01:07:54,080
 and you have dirty blocks into the buffer cache? Obviously, this information is lost,

479
01:07:55,360 --> 01:08:02,240
 right? And now you have the case you have from your application, you wrote something in the file,

480
01:08:02,240 --> 01:08:13,600
 it was open and now machine fails after you wrote. And what you wrote is not in the file.

481
01:08:13,600 --> 01:08:19,520
 Although as a programmer, you may believe that, well, I wrote in a file, the file is persistent,

482
01:08:19,520 --> 01:08:29,440
 it must be in the file because it happened before the failure. And things can be pretty ugly.

483
01:08:29,440 --> 01:08:35,280
 If you, for instance, you only modify a directory, if you don't modify, then you may lose entire

484
01:08:35,280 --> 01:08:41,040
 files because you lost their dangling files, which have no presence in the directory.

485
01:08:44,000 --> 01:08:52,560
 Or there are many others. You may lose inodes and so forth. So the file system needs some

486
01:08:52,560 --> 01:08:58,560
 mechanisms to ensure the persistence, including some recovery mechanisms to recover

487
01:08:58,560 --> 01:09:09,840
 kind of inconsistent state. So there are three important properties of a system when it comes

488
01:09:12,720 --> 01:09:15,920
 to its, so to speak,

489
01:09:15,920 --> 01:09:31,440
 performance availability and so forth of a system and the file system in particular.

490
01:09:31,440 --> 01:09:38,000
 So there are three things. One is availability and availability really tells you

491
01:09:40,160 --> 01:09:46,000
 percentage of time while the system can accept and serve your requests.

492
01:09:46,000 --> 01:09:58,000
 Okay. So three lines, meaning in this case, meaning that the probability you send a request

493
01:09:58,000 --> 01:10:03,360
 to the file system and you don't get a reply or it is not resolved is 0.1%.

494
01:10:05,440 --> 01:10:12,400
 Then durability. The durability means is that if the data is on the disk,

495
01:10:12,400 --> 01:10:21,360
 then it's durable, meaning that if a disk fails, there is still a way to recover the data.

496
01:10:21,360 --> 01:10:30,400
 Availability and durability, they are not the same because availability means that the data

497
01:10:30,400 --> 01:10:38,720
 is somewhere, but doesn't mean necessarily that it's accessible. And then is reliability,

498
01:10:38,720 --> 01:10:45,600
 which is usually the strongest one, is the ability of a system to perform its required functions as

499
01:10:45,600 --> 01:10:52,880
 it is defined and its application level definition, what that means to perform the function.

500
01:10:52,880 --> 01:10:59,440
 And it's typically stronger because it implies availability. If it's not available,

501
01:11:00,000 --> 01:11:13,200
 the system cannot perform its functions and also includes durability because the data has to be,

502
01:11:13,200 --> 01:11:17,520
 any persistent data, should be able to get access to it.

503
01:11:19,840 --> 01:11:30,000
 Okay. So first we are going to look about how to make this system, the file system durable.

504
01:11:30,000 --> 01:11:42,000
 How do you make a file system durable? What I said that what I want to ensure here is that if

505
01:11:42,000 --> 01:11:54,960
 I wrote the data on a disk, in a file, I want to be able to read that data at a later time,

506
01:11:54,960 --> 01:12:00,000
 no matter what, no matter what kind of failures were happening.

507
01:12:00,000 --> 01:12:11,280
 Right? So say the one classic failure is a disk failure. I wrote the data on the disk,

508
01:12:11,280 --> 01:12:17,200
 it made it from the buffer cache to the disk, but then that's dispelled. I wrote the data.

509
01:12:17,200 --> 01:12:23,040
 How do you avoid that?

510
01:12:23,040 --> 01:12:35,680
 Redundancy, you back up. Absolutely. It's what you are doing for your show or you should do for your

511
01:12:36,240 --> 01:12:42,880
 own, the most important data you have. Write to more than one disk, totally.

512
01:12:42,880 --> 01:12:58,000
 Okay. And there are many ways to do it. And we'll see, you can do it, read Solomon,

513
01:12:59,360 --> 01:13:06,800
 you're correcting codes. There are other things you can do. It is like you can have battery backed

514
01:13:06,800 --> 01:13:12,880
 RAMs or non-volatile RAM. So it doesn't necessarily need to be on the disk, but you can also write it

515
01:13:12,880 --> 01:13:21,600
 in RAM as long as it's powered as in the own battery, so when the machine fails, the RAM is

516
01:13:21,600 --> 01:13:32,560
 not wiped out. But to really to have the data surviving more, a lot of more long term,

517
01:13:32,560 --> 01:13:37,760
 we need to replicate it. And again, when you replicate it, you need to be very clear about the

518
01:13:37,760 --> 01:13:46,160
 assumptions, failure assumption you are making. Because if you have two disk drives

519
01:13:46,160 --> 01:13:54,640
 and both are on the same machine, if the machine fails, then you lost both copies.

520
01:13:54,640 --> 01:14:02,400
 To avoid that, you need to put copies on different machines. But if the machines are in the same rack

521
01:14:02,400 --> 01:14:08,480
 and the rack fails or the data center experienced a catastrophic failure,

522
01:14:09,920 --> 01:14:16,480
 you lost access to those copies. Now you can put the copies on several different continents,

523
01:14:16,480 --> 01:14:23,120
 you are going to be safer there. And you see that, right? Because typically, many of you assume

524
01:14:23,120 --> 01:14:29,600
 that when they back up their data, where do you back up the data? Do you back on the same machine

525
01:14:29,600 --> 01:14:40,400
 now? Where do you back it up? Actually, I'm curious. Anyone back up the data?

526
01:14:40,400 --> 01:14:51,520
 Yes, cloud. Many people bring their phone to clone the cloud.

527
01:14:53,120 --> 01:15:02,880
 Yeah, and Michael has a good point. Maybe also as a medium,

528
01:15:02,880 --> 01:15:08,640
 you want to write on different media to protect against some catastrophic failure,

529
01:15:08,640 --> 01:15:13,600
 which is going to involve only on media or degradation over time.

530
01:15:13,600 --> 01:15:22,560
 But many people write to the cloud. And the cloud actually can replicate the data

531
01:15:22,560 --> 01:15:25,520
 across different regions in different continents.

532
01:15:25,520 --> 01:15:35,440
 But now, getting back, and this was many years back, and this was

533
01:15:35,440 --> 01:15:41,200
 people who developed first the technology called RAID, actually developed at Berkeley

534
01:15:41,200 --> 01:15:50,800
 by David Patterson, Randy Katz and their student Gar Gibson, who was faculty at Carnegie Mellon,

535
01:15:50,800 --> 01:16:00,080
 now it's at Toronto. And you just have a replica for each disk and one you call recovery

536
01:16:00,080 --> 01:16:12,160
 group. And you double the capacity you need. And when you write, write is done only if you write

537
01:16:12,160 --> 01:16:21,200
 both at the primary and the recovery group. And that's it, right?

538
01:16:21,200 --> 01:16:27,280
 Now, the problem here is that the writes can be quite expensive because each write

539
01:16:27,280 --> 01:16:37,680
 it needs to go to two disks. And you remember when you write, you have rotation disk latency.

540
01:16:38,400 --> 01:16:45,760
 Well, you are going now to finish and you are going to wait for the slowest one to finish disk,

541
01:16:45,760 --> 01:16:53,280
 for the slowest disk to finish. That was a performance is worse for writes. Of course,

542
01:16:53,280 --> 01:17:00,000
 you can synchronize the disks, right? So then all both of them finish at the same time.

543
01:17:00,000 --> 01:17:01,920
 But that's very hard.

544
01:17:04,720 --> 01:17:10,880
 What about the reads? Well, it turns out that with reads you are better, right? Because

545
01:17:10,880 --> 01:17:16,560
 with reads you can actually, you can have twice the bandwidth because you can read for

546
01:17:16,560 --> 01:17:27,120
 any replica. Right? Or you can send the same read to both replicas and you get the one which is

547
01:17:27,120 --> 01:17:35,440
 arrived first. So it's good for reads, both in terms of latency and the throughput is bad for

548
01:17:35,440 --> 01:17:45,760
 writes. And also you need to double the capacity. What happens on the recovery when a disk fails?

549
01:17:45,760 --> 01:17:50,320
 You need to buy a new disk and then you are going to copy the data to the new disk.

550
01:17:50,320 --> 01:17:55,440
 Write all data to the new disk. So now you have again two replicas.

551
01:17:55,440 --> 01:18:05,520
 Now that was again pretty expensive. And that was RAID 1. Again, RAID

552
01:18:05,520 --> 01:18:18,800
 means redundant arrays of inexpensive disks, as opposed to much more expensive, presumably more

553
01:18:18,800 --> 01:18:24,800
 reliable disks. Here you have cheap, less reliable disks, but because you do this kind of replication

554
01:18:24,800 --> 01:18:31,520
 or now you see razor coding, you are going to achieve a much higher reliability, given a much

555
01:18:31,520 --> 01:18:41,040
 higher reliability than an expensive disk. So, and there are different again levels of RAID.

556
01:18:41,040 --> 01:18:51,120
 And you can look in the textbook. But RAID 5 and more, what they do, they basically,

557
01:18:52,240 --> 01:19:03,360
 they stripe the data across multiple disks and they add also a parity disk. A parity block.

558
01:19:03,360 --> 01:19:15,520
 OK, so basically say in this case, you know, in this figure that each D, D1, D2, D3, these are

559
01:19:17,360 --> 01:19:30,800
 disks and these are the blocks which you could, so D0, D1, D2, D3, sorry, these are the blocks,

560
01:19:30,800 --> 01:19:37,600
 the name of the blocks, D0, D1, D2, D3. And then to this one, you are going to add a parity block,

561
01:19:38,240 --> 01:19:48,640
 P0. And the one you can do it, the P0 is the XOR of the values of the data blocks. Right?

562
01:19:48,640 --> 01:19:55,200
 So therefore, you have four data blocks, you are going to add another one, which is a parity one.

563
01:19:55,200 --> 01:20:04,720
 So the overhead here is 25%, not doubled. Right? And what happens here, you can, if one disk goes

564
01:20:04,720 --> 01:20:13,760
 away, you can still reconstruct the data. And how do you do it? You reconstruct the data of the lost

565
01:20:13,760 --> 01:20:22,240
 block by simple XORing. All the remaining blocks, the three remaining blocks, plus the parity block.

566
01:20:24,240 --> 01:20:36,240
 It's as simple as that. Okay? And you can also spread the information not only across multiple

567
01:20:36,240 --> 01:20:41,840
 disks in the same server or in the same data center, you can also do it across the internet.

568
01:20:41,840 --> 01:20:51,280
 Right? So you can use the same technique. Okay, so I'm going to stop here. We are going to talk

569
01:20:51,280 --> 01:21:04,560
 a little bit more about this, how to make this durable. And next time. Okay, thank you. And this

570
01:21:04,560 --> 01:21:11,440
 Thursday, as you know, it's Veterans Day. So I hope that you are going to enjoy a well-deserved

571
01:21:13,360 --> 01:21:30,640
 lighter day. And you can take, you can get some rest. Okay, I'll see you next week. Bye.

