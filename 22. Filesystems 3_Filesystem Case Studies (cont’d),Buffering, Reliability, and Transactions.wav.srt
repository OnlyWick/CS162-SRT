1
00:00:00,000 --> 00:00:29,840
 Hello everyone.

2
00:00:29,840 --> 00:00:34,160
 Welcome. Today we are going to continue our discussion on file systems.

3
00:00:34,160 --> 00:00:43,360
 And if you remember last time, we are discussing about, we ended our lecture with a discussion of

4
00:00:43,360 --> 00:00:53,920
 the fast file systems. And we are, which was the version, the next version of the file system in

5
00:00:53,920 --> 00:01:04,880
 Unix BSD after original version. And this has a lot of, quite a few innovations. And we went over

6
00:01:04,880 --> 00:01:16,160
 a few last time, mostly the innovation or about improving the performance like different block

7
00:01:16,160 --> 00:01:26,560
 sizes, you know, basically improving the rotation latency and a few more features to again, to

8
00:01:26,560 --> 00:01:33,600
 improve the performance. The one thing we haven't, we didn't go over is the organization of the

9
00:01:33,600 --> 00:01:41,520
 directories. So what is the, what is the problem here? So assume that you have a very large

10
00:01:41,520 --> 00:01:49,600
 directory with many entries. Now, if you remember the early file systems organized directories as

11
00:01:49,600 --> 00:01:58,880
 lists, this was in the case of FAT, the Microsoft DOS first file system,

12
00:01:58,880 --> 00:02:09,200
 or arrays and of entries or an entry is a file name and the inode corresponding to that file name.

13
00:02:10,640 --> 00:02:17,040
 And I know if you remember contains the entire information about where,

14
00:02:17,040 --> 00:02:24,960
 which are the blocks of that particular file, as well as other information about that file, like

15
00:02:24,960 --> 00:02:32,640
 access rights and things like that. The time was on last updated and other information.

16
00:02:34,000 --> 00:02:40,880
 So the challenge with that is that say, I want to locate one file, right? And whenever you are going

17
00:02:40,880 --> 00:02:49,280
 to, and there's a common line, or you are going to write a file names to say, see the contact to open,

18
00:02:49,280 --> 00:02:56,720
 you need to find that file name because in the directory, because the entry in the directory of

19
00:02:56,720 --> 00:03:02,080
 that file name, because that entry will give you the inode and the inode will tell you where are

20
00:03:02,080 --> 00:03:10,320
 the data blocks on the disk of that file. But if the directory is organized like a list or an

21
00:03:10,320 --> 00:03:18,800
 array of entries, then the only way to locate that file is linear search. If the directory is very

22
00:03:18,800 --> 00:03:28,960
 large, this linear search is expensive. Even more, you may need to read the entire directory just to

23
00:03:28,960 --> 00:03:36,080
 find the file. And again, if the file, if the directory is large, you may have to load all the

24
00:03:36,080 --> 00:03:44,080
 blocks which are storing this large directory, again, just to find the file. Okay. So what is

25
00:03:44,080 --> 00:03:50,800
 the solution here? Of course, the solution is to find a better structure than a list of an array.

26
00:03:51,600 --> 00:03:59,040
 And if you want to search, what are some better structure to search than a list or an array?

27
00:03:59,040 --> 00:04:08,800
 You learn about many of those. Can you give an example? What would you do if you were to design

28
00:04:08,800 --> 00:04:17,280
 the directory data structure, which is much more efficient in terms of search? It's a binary tree,

29
00:04:17,280 --> 00:04:28,080
 right? Okay. Yeah, that's pretty good. And it's even better here, we can have a B-tree. Okay. And

30
00:04:28,080 --> 00:04:36,400
 when you have a tree, a search tree, the depth of the tree is what gives you the complexity of

31
00:04:36,400 --> 00:04:47,040
 locating that particular, in our case, file, which is a leaf in that tree. So this is for you,

32
00:04:47,040 --> 00:04:58,080
 just a B-tree, it's a more sophisticated search tree. And if you remember, this is just a reminder

33
00:04:58,080 --> 00:05:12,240
 for you to refresh your memory. So here, a node is pointing out at a lower level to an array. And

34
00:05:12,240 --> 00:05:19,680
 that array contains values, which are smaller than the value of the pattern node. So in this case,

35
00:05:19,680 --> 00:05:30,160
 100 points to this array of values, and all these values 48, 50, 79 are less than 100.

36
00:05:30,160 --> 00:05:41,120
 Also the following node, the sibling node of 100, 155, now is going to point to a set of values,

37
00:05:41,120 --> 00:05:48,240
 to an array of values, which are lower than itself, but higher than its sibling on the left,

38
00:05:48,240 --> 00:05:55,920
 right? So these are numbers between 128 and 140 and so forth. Okay.

39
00:05:55,920 --> 00:06:05,440
 Eventually, you need to get to the leafs and let's see how to find a value at the leaf.

40
00:06:06,080 --> 00:06:11,600
 So let's see how this is used in the context of directories. So in the context of directories,

41
00:06:11,600 --> 00:06:18,000
 how do we get a number? Well, the number is easy to get, you just hash the name of the file. So now

42
00:06:18,000 --> 00:06:23,120
 we get a number and hopefully the hash is good enough. So the probability of collisions is very

43
00:06:23,120 --> 00:06:30,720
 small. Okay. But this is what it is. It's exactly, it's a binary search tree, sorry, it's a B-tree,

44
00:06:31,280 --> 00:06:40,800
 in which the values in the B-trees are the hashes of the file names.

45
00:06:40,800 --> 00:06:51,280
 And you can see here on the leafs, you are going to see to have the entries in the directory,

46
00:06:51,280 --> 00:06:58,880
 which again is a name of the file, the name and the file number or inode.

47
00:07:01,040 --> 00:07:17,840
 Any questions? Okay. So in this particular case, you see that the hash to out to it's C194,

48
00:07:17,840 --> 00:07:23,920
 and you are looking through the B-trees. First of all, you are going to look at the values,

49
00:07:24,640 --> 00:07:32,480
 which are, you are looking to the values, to the consecutive values as that the value on the left

50
00:07:32,480 --> 00:07:41,600
 is lower, the value on the right is higher than C194. And then you are going to pick the node

51
00:07:41,600 --> 00:07:51,120
 corresponding to that, which is for the range. And then you are going to go on level down. So for

52
00:07:51,120 --> 00:07:59,040
 instance, in this case, C194 is smaller than the first value at the first level. So you are going

53
00:07:59,040 --> 00:08:13,520
 to go take and go to the arrays pointed out by that value node, AD1102. Okay. Because remember,

54
00:08:13,520 --> 00:08:23,440
 AD1102, which is a leftmost node here at the top level is going to point only to values which are

55
00:08:23,440 --> 00:08:31,280
 lower than itself. Okay. So you are going to go to the next level, the next level, the second level.

56
00:08:31,280 --> 00:08:40,160
 Again, you are going to look at the values at that node, and the values are going to go from

57
00:08:40,160 --> 00:08:48,800
 the smallest value is C195, which again C195 is greater than C194. So you are going to go again

58
00:08:48,800 --> 00:08:56,560
 to the next level, it's going to be a node, which is pointed out by the first,

59
00:08:56,560 --> 00:09:04,960
 corresponding to the leftmost value. Okay. And finally here, you are just going to see that

60
00:09:05,520 --> 00:09:11,600
 you are going to have to find C194, you'll find a match, and that match will take you to the

61
00:09:11,600 --> 00:09:18,400
 entry in the file directory, which is the name of the file and the file number.

62
00:09:18,400 --> 00:09:32,160
 Okay. So Michael here has a great question. Why do we want to hash the file names? Wouldn't using

63
00:09:32,160 --> 00:09:38,240
 the file name themselves in lexicographical order have the benefit that it's easy to enumerate files

64
00:09:38,240 --> 00:09:49,920
 in lexicographical order? Anyone wants to answer this question?

65
00:09:49,920 --> 00:10:09,520
 Yes. The file names can have arbitrary length, so it's going to make the implementation a little bit

66
00:10:09,520 --> 00:10:17,760
 harder. Someone says also perhaps to distribute the hash is uniform, that's a great point,

67
00:10:18,640 --> 00:10:26,800
 as well. So you get a more balanced tree. If you have a hash, a good hash is going to give

68
00:10:26,800 --> 00:10:33,200
 you pseudo random numbers. So you are going to easily get a balanced tree once you build it.

69
00:10:33,200 --> 00:10:39,360
 But the other thing is that if you use a file names, it's strings. So comparing two strings

70
00:10:39,360 --> 00:10:46,560
 is much more expensive than comparing two numbers. Okay. So again, remember we are doing building

71
00:10:46,560 --> 00:10:53,120
 bit trees because we want to improve the performance. So hashes give you values,

72
00:10:53,120 --> 00:11:00,160
 which at the end of the day can be our integers. And just comparing integers is much easier,

73
00:11:00,160 --> 00:11:08,240
 it's much faster and also much easier to build data structures where the fields have

74
00:11:09,040 --> 00:11:19,440
 unique with the same length. Do different file systems use different bit tree types or is some

75
00:11:19,440 --> 00:11:26,400
 standard like 2-3-4? That's a great question. Actually, I do not know the answer to that

76
00:11:26,400 --> 00:11:35,120
 question. I should look around. But it's a good question. I would assume that there are some

77
00:11:36,000 --> 00:11:39,440
 rule of thumb here about what are the good numbers for this application.

78
00:11:39,440 --> 00:11:49,840
 While directory can be large, it doesn't compare with the number of rows you can have in a table.

79
00:11:49,840 --> 00:11:56,560
 As you probably know, bit trees are coming from database field to index the rows in a table.

80
00:11:56,560 --> 00:12:02,400
 So I guess you can have hundreds of millions or even billions of rows in a table.

81
00:12:02,960 --> 00:12:08,160
 I don't know the number of files can be large is not going to reach that value.

82
00:12:08,160 --> 00:12:19,360
 But yeah, that's a good question. Okay. So now we switch gears and we are going to look at

83
00:12:19,360 --> 00:12:26,480
 another file system and file system is Windows NTFS. What NTFS stands for?

84
00:12:29,120 --> 00:12:38,080
 New technology. So it's called New Technology File System. It's an interesting name because

85
00:12:38,080 --> 00:12:44,880
 it's always going to be called new technology. But of course, now this is like 30, 20, 30 years old.

86
00:12:44,880 --> 00:12:53,920
 So it cannot be that so new technology. And this is still the default on modern Windows systems.

87
00:12:54,560 --> 00:13:02,080
 And if you are using a Windows machine, most likely you are using an NTFS file system.

88
00:13:02,080 --> 00:13:10,080
 And let me just tell you what is the main idea here. The main idea here, one of the main ideas

89
00:13:10,080 --> 00:13:21,840
 is that instead of fixed blocks, we have variable length blocks. And these variable length blocks

90
00:13:21,840 --> 00:13:34,240
 are called extents. Okay. And then instead of the FAT or INOD array, we are having here what is

91
00:13:34,240 --> 00:13:41,040
 called a master file table. And the master file table is almost like a database. And it's in the

92
00:13:41,040 --> 00:13:49,920
 database because it has attribute value pairs and these attributes and the values can be different

93
00:13:49,920 --> 00:14:01,200
 things. So and each of these entries, which is attribute value, can have one kilobyte size.

94
00:14:01,200 --> 00:14:11,440
 So it's more general than the name, file name pairs, which you are finding in traditional file

95
00:14:11,440 --> 00:14:24,160
 systems like we learn so far. And let me show you a little bit how this is organized. But the point

96
00:14:24,160 --> 00:14:34,240
 is that the reason it's more complex, but this complexity buys you efficiency and performance.

97
00:14:34,960 --> 00:14:49,040
 And you'll see how. One is if you can find enough room on the disk and you can have an extent,

98
00:14:49,040 --> 00:14:59,760
 let's say of 100 kilobytes larger than any block. So 100 kilobytes extend or even higher

99
00:14:59,760 --> 00:15:10,000
 and just contiguous. Right. So it's very fast to read that block. There is no seek that extent.

100
00:15:10,000 --> 00:15:16,000
 There is no seek time. You have only one seek time you need to waste in order to get to that

101
00:15:16,000 --> 00:15:24,400
 extent. But then you read 100 kilobytes. With blocks, it's again, it's like they may not be

102
00:15:24,400 --> 00:15:35,120
 contiguous. So you are going to have more seek times and it's more complex. And not only that,

103
00:15:35,120 --> 00:15:41,520
 but as you'll see, like for instance, if you have a small file which is more than one kilobyte,

104
00:15:41,520 --> 00:15:46,640
 you can actually put in this master file table because one of these entries, you remember,

105
00:15:48,000 --> 00:15:58,640
 it can be up to one kilobyte. OK, and let's look. So this is a master file table

106
00:15:58,640 --> 00:16:07,600
 and then you have a bunch of records and these records are content depends on how large is a file.

107
00:16:07,600 --> 00:16:14,160
 You also are going to learn next lecture about journaling, which is a way to provide

108
00:16:14,160 --> 00:16:22,320
 file reliability for the files. OK, so this is a very small, it's a small file, a tiny file.

109
00:16:22,320 --> 00:16:29,360
 So the file is less than one kilobyte. So one of these entries is one kilobyte. So what do you have

110
00:16:29,360 --> 00:16:35,280
 here? You have in one of these entries in the master file table for these small files, you have

111
00:16:35,280 --> 00:16:42,800
 standard information, which is the usual, you know, that when the file was created or modified

112
00:16:42,800 --> 00:16:50,720
 the last time, maybe you want to access the last time, owner ID, some security specifiers, flags,

113
00:16:50,720 --> 00:16:56,080
 whether it's read only, hidden, whether you can see, like if you do like a user,

114
00:16:56,080 --> 00:17:01,760
 Alice, whether you see the file or not, and or it's a system file and things like that. Right.

115
00:17:01,760 --> 00:17:05,680
 So this is what you have in the standard information. Then you have the file name.

116
00:17:05,680 --> 00:17:12,000
 OK, so in the file table, master file table, you have the file name. Remember that

117
00:17:12,000 --> 00:17:18,400
 in the case of inode array, you do not have, you have only inode, the inode, which is the file

118
00:17:18,400 --> 00:17:24,560
 number. The file name is in the directory only. And then like we discussed, if the data is a

119
00:17:24,560 --> 00:17:31,040
 file is small, you may be able to put the entire data in this entry. Right. And then it's free.

120
00:17:31,040 --> 00:17:39,600
 And you are going to just read all one of these, all this record in one go. Right. So you're with

121
00:17:39,600 --> 00:17:45,600
 one disk access. You can read everything about that file, including the data. So it's as fast

122
00:17:45,600 --> 00:17:54,960
 as you can get. Right. So now what happens if the file is a little bit larger? If the file is larger,

123
00:17:54,960 --> 00:18:02,240
 then instead of having the data here, right, you have a bunch of pointers, attribute values.

124
00:18:02,240 --> 00:18:08,720
 Remember the same attribute values. So you are going to have and each of those is going to point

125
00:18:08,720 --> 00:18:18,000
 out to a data extent. Again, remember data extent, it's a variable size block. So in order to refer

126
00:18:18,000 --> 00:18:29,360
 to a variable size block or extent, you are going to have a start, a starting pointer and the length

127
00:18:29,360 --> 00:18:40,560
 of the extent. Right. That's enough. Okay. So this is what you have. Now, if you have other even

128
00:18:40,560 --> 00:18:51,280
 larger files, then what you have, you can have pointers to other extents. Remember, to the other

129
00:18:51,280 --> 00:19:02,080
 MTF records. So and remember at the end of the day, extent is a contiguous region on the disk.

130
00:19:02,080 --> 00:19:12,880
 And so these are, you know, this leaf now, so to speak, in this hierarchy are where the data

131
00:19:12,880 --> 00:19:20,960
 extends. Okay. So in this particular case, the attribute list is going to point to have to point

132
00:19:20,960 --> 00:19:38,720
 to different entries in the master file table. And this is for a huge file. It's again, you are going

133
00:19:38,720 --> 00:19:51,200
 to have a lot of more empty MFT records. So here we go. Right. So basically at one level to summarize,

134
00:19:51,200 --> 00:20:03,040
 if the file is very small, the entire file can fit into an MFT record. And if the file is a little

135
00:20:03,040 --> 00:20:13,360
 bit larger, then you still have on one of the MFT record, but instead of data, you have a bunch of

136
00:20:13,360 --> 00:20:22,320
 pointers to data extents. So it's one level of indirection. Right. And the extents are referred

137
00:20:22,320 --> 00:20:32,480
 by the start and the length. If the file is even larger, then you are going to have multiple MFT

138
00:20:32,480 --> 00:20:38,720
 records. And if the file is even larger, you are going to have more MFT records.

139
00:20:38,720 --> 00:20:49,920
 Okay. Directories are implemented as bit trees like we learn. The file numbers identify its

140
00:20:49,920 --> 00:20:59,360
 entry in MFT. It is similar with the inode table or array. And MFT entry always has a file name

141
00:20:59,360 --> 00:21:09,600
 attribute. Right. Remember here, right. So you can check it. It's human readable as well. How do you

142
00:21:09,600 --> 00:21:16,240
 implement hardlink? Very simple. The way you are going to implement a hardlink, that will be another

143
00:21:16,240 --> 00:21:31,120
 entry in the MFT table, which is going to point to the same data extents or is going to point to

144
00:21:31,120 --> 00:21:42,640
 another MFT which represents the file. It's attribute list part of the data. This was a

145
00:21:42,640 --> 00:21:50,320
 question. Very good. I assume that where it's attribute list, I think it's next.

146
00:21:50,320 --> 00:21:59,360
 Yes. The attribute list, it's again, it's part of the MFT record.

147
00:21:59,360 --> 00:22:07,040
 I'm not sure what you mean, what the question means about asking about whether it's part of

148
00:22:07,040 --> 00:22:15,760
 the data. It's part of an MFT record for that file. And it's again, you have this attribute

149
00:22:15,760 --> 00:22:28,000
 list if you cannot fit everything in a single MFT record, where instead of data, you are going

150
00:22:28,000 --> 00:22:36,720
 to have pointers to the data extents. So you cannot have this one. So here in the data part,

151
00:22:37,280 --> 00:22:44,160
 you do not have enough room to store the pointers to all extents.

152
00:22:44,160 --> 00:23:04,240
 Okay, that's great. So it's a question here from Akshay. What are the pros and cons of this,

153
00:23:04,240 --> 00:23:13,840
 I assume NTFS over Berkeley FAST file system? The list of extents seems similar to the list of

154
00:23:13,840 --> 00:23:21,680
 direct and direct pointers in the inode. So the data part is replaced with an attribute list of

155
00:23:21,680 --> 00:23:31,200
 larger files. Yeah. Do you want to, anyone would like to answer?

156
00:23:31,200 --> 00:23:44,560
 So Gilbert, you mentioned about the data part is replaced with the attribute list of

157
00:23:44,560 --> 00:23:48,720
 four large files. I'm not sure is that an answer to Akshay's question?

158
00:23:58,080 --> 00:24:04,400
 So anyway, who would like to try to answer this Akshay question?

159
00:24:04,400 --> 00:24:18,080
 What are the pros and cons between NTFS and FFS?

160
00:24:22,080 --> 00:24:42,080
 So one thing is about, remember, what is the difference between extent and blocks reviews in FFS?

161
00:24:50,240 --> 00:24:56,480
 Okay, so with extents, you can have any size blocks.

162
00:24:56,480 --> 00:25:04,240
 What FFS gives you is the ability to configure the file system with different block sizes.

163
00:25:04,240 --> 00:25:07,920
 But once you configure the block sizes will still be the same,

164
00:25:07,920 --> 00:25:11,360
 the same size, all the blocks are going to be the same size.

165
00:25:13,760 --> 00:25:22,960
 Right? And with a data extent, with arbitrary size, you are going to be guaranteed that in

166
00:25:22,960 --> 00:25:28,720
 order to retrieve and to read all the data from one extent, you are going to have only need,

167
00:25:28,720 --> 00:25:33,440
 only one access, only one SIC to get access to that data.

168
00:25:33,440 --> 00:25:42,560
 The blocks are going to be in general smaller than one extent. So you are going to have multiple

169
00:25:42,560 --> 00:25:47,120
 blocks and multiple blocks means that you may have to, if they are not contiguous and you are

170
00:25:47,120 --> 00:25:53,840
 not guaranteed to be contiguous if they are blocks, you may have to pay multiple access times to

171
00:25:53,840 --> 00:26:03,920
 read the same amount of data. NFS is also better for small files, exactly, Michael.

172
00:26:06,720 --> 00:26:16,400
 And because like you see here, for if the file is very small, it's less than one kilobyte,

173
00:26:16,400 --> 00:26:27,440
 then it fits in one MFT record. Well, in the previous case, you need to have multiple accesses

174
00:26:27,440 --> 00:26:34,800
 to load a small file, at least, you know, you need to go to read from inode,

175
00:26:35,760 --> 00:26:42,560
 you need to go to read the data block, which is separate. Like this is equivalent,

176
00:26:42,560 --> 00:26:51,360
 like the data itself will be part of the inode. Right? But with FFS is not. So you need to read

177
00:26:51,360 --> 00:26:58,400
 the inode, which is access. And then even if you don't have very tiny file, you still need to read

178
00:26:58,400 --> 00:27:13,200
 another block where the data is located. Okay. Okay, excellent. Okay, now let's move and do

179
00:27:13,200 --> 00:27:22,640
 something even more interesting. Memory mapped files. And let's, what are the memory map files?

180
00:27:23,200 --> 00:27:29,440
 You see, and why do we need those? You see, when you access a file, and what we learn,

181
00:27:29,440 --> 00:27:35,440
 things are getting quite complicated. And the data is replicated multiple places from that file,

182
00:27:35,440 --> 00:27:40,160
 you need to the operating system has to have a buffer, we are going to learn more about the

183
00:27:40,160 --> 00:27:45,280
 buffer a little bit later, to store the data in that buffer. And then there is another buffer to

184
00:27:45,280 --> 00:27:51,680
 put the data at the application level. Okay, so you have multiple copies of the data plus system

185
00:27:51,680 --> 00:28:02,800
 costs. Okay. But what if we could map an entire file directly to the memory? Right? And when you

186
00:28:02,800 --> 00:28:09,840
 are going to read something from that file, it just is like you read from that region of memory.

187
00:28:09,840 --> 00:28:16,880
 And when you have to write it, you write it like in memory. And eventually the operating system

188
00:28:16,880 --> 00:28:26,720
 will take care to write back your changes to the disk. Right? By the way, the executable files are

189
00:28:26,720 --> 00:28:35,920
 treated this way when we are executing a process. So now, to see the differences, let's refresh our

190
00:28:35,920 --> 00:28:44,960
 memory about what happens on a page fault. So you have an instruction, the instruction provides a

191
00:28:44,960 --> 00:28:53,360
 virtual address, the virtual address goes through MMU and you are going to get the physical page

192
00:28:53,360 --> 00:29:01,440
 number. And then you are going to put the page number, the physical page number is going to index

193
00:29:01,440 --> 00:29:09,680
 into a page table. Right? And the page table is going to give you the frame number.

194
00:29:10,640 --> 00:29:17,520
 And the frame number is going to be, depending whether you are going to have paging or segmentation,

195
00:29:17,520 --> 00:29:22,080
 you are going to concatenate the frame number with the offset and you are going to get the address

196
00:29:22,080 --> 00:29:28,400
 of the physical number. Okay. So you go virtual address, you get the page number from the virtual

197
00:29:28,400 --> 00:29:33,600
 address, it's going to point in the page table, it's going to give you the frame number, and then

198
00:29:33,600 --> 00:29:39,280
 you are going to get the offset of the virtual address and you are going to get the frame and

199
00:29:39,280 --> 00:29:46,400
 offset concatenating, it's going to give you the address in the physical memory. Right?

200
00:29:46,400 --> 00:29:54,560
 Now, if you are going to have a page fault, page fault means that you do not find the entry

201
00:29:54,560 --> 00:29:59,680
 corresponding to the page number in the page table. So what we are going to do is there is an

202
00:29:59,680 --> 00:30:04,880
 exception, which is going to be handled by the operating system in particular by the page fault

203
00:30:04,880 --> 00:30:14,560
 handler. And that is going to locate on the disk what the page and is going to load it in memory.

204
00:30:14,560 --> 00:30:21,040
 Maybe for loading that page, you need to pick an existing page because maybe the memory is full.

205
00:30:21,040 --> 00:30:29,120
 And then you are going to update the page table to point to the page, which you just

206
00:30:29,120 --> 00:30:37,520
 loaded in memory. And then you return from the page fault and the operating system will reschedule

207
00:30:37,520 --> 00:30:43,760
 that process, which causes a page fault, re-execute the same instruction. And when

208
00:30:43,760 --> 00:30:50,560
 you re-execute the same instruction, now you do have page table is correct. So you are going to

209
00:30:50,560 --> 00:30:54,640
 be able to read the page from the physical memory.

210
00:30:57,200 --> 00:31:03,280
 OK, so we did that in the past. This is just a refresher. So now what happens is the memory

211
00:31:03,280 --> 00:31:09,760
 map files. Well, very simple. It's exactly what you would expect. Here is a file on the disk.

212
00:31:09,760 --> 00:31:16,720
 So you are going just to map the file in the memory and you are going to map to initialize

213
00:31:16,720 --> 00:31:22,400
 the page table. You are going to allocate page table entries for each page of the file.

214
00:31:24,880 --> 00:31:35,280
 And then when you have in this case a page fault, basically when you are going that the page is not

215
00:31:35,280 --> 00:31:42,240
 in memory, which represent a file, which is part of the file, then you are going to load directly

216
00:31:42,240 --> 00:31:53,920
 the file, the portion of the file into the page. And now you are just going to read files when you

217
00:31:53,920 --> 00:31:59,280
 access it, when you access a physical memory, you basically access the content of the file.

218
00:31:59,280 --> 00:32:10,480
 OK. So this is a and I'll give you an example, some of which will clarify things.

219
00:32:10,480 --> 00:32:20,240
 This is a system call to create a memory map file. And this basically you provide the file descriptor

220
00:32:20,240 --> 00:32:26,880
 of the file and the offset from where you want to get to map the data from the file. Right.

221
00:32:26,880 --> 00:32:37,520
 So, for instance, if offset is 100, this means that I want to map starting with a byte 100

222
00:32:37,520 --> 00:32:47,040
 from the file into memory. These are some flags and protection. The other very important fields

223
00:32:47,040 --> 00:32:54,800
 are the address and the length. The length is how much from the file you want to map in memory.

224
00:32:54,800 --> 00:33:01,120
 And the address, you can even give an address. And if you give an address, then the file system

225
00:33:01,120 --> 00:33:11,040
 is going to try to map the file at the address it is given. But if it cannot, then it may find

226
00:33:11,040 --> 00:33:18,640
 another place in the physical memory with enough space to map the file, which basically is a

227
00:33:18,640 --> 00:33:28,880
 contiguous region of length length. And in either way, the function returns the address in the

228
00:33:28,880 --> 00:33:42,000
 memory where you map the file. OK. In most of the cases, you can use also this memory map

229
00:33:42,000 --> 00:33:46,400
 also to communicate between processes. We'll talk briefly about that later.

230
00:33:46,400 --> 00:33:55,840
 But here is an example of a program to just give you a sense of what memory map files

231
00:33:56,400 --> 00:34:07,680
 means and what is there in effect. So here is a simple program. And just for the sake of it,

232
00:34:07,680 --> 00:34:14,480
 we are going to type three addresses to bring three addresses. One is something, is the address

233
00:34:14,480 --> 00:34:23,440
 of something, this variable something. And that something is when it's a data, right?

234
00:34:24,240 --> 00:34:32,240
 It's a data segment, right? Then we are going to allocate something. And everything which is

235
00:34:32,240 --> 00:34:40,800
 allocated, you know it's on the heap. So this is address for the result of MLK. It resides in the

236
00:34:40,800 --> 00:34:48,880
 heap. And then the last one is you have this my file here variable, which is a local variable.

237
00:34:48,880 --> 00:34:55,200
 And we know that the local variables of the functions are put on the stack. So this address

238
00:34:55,200 --> 00:35:02,640
 refers on-- it's an address in the stack segment. Then you open a file. And this is a file we want

239
00:35:02,640 --> 00:35:12,240
 to map in the memory. And now we map the file in the memory. So basically, we are taking this file

240
00:35:13,920 --> 00:35:20,800
 descriptor. We are going to pass to a map the file descriptor my_fd. And zero, meaning that I want to

241
00:35:20,800 --> 00:35:28,240
 start mapping from the first byte in the file. And here there are again some flags about read,

242
00:35:28,240 --> 00:35:33,120
 write. And this is about whether this can be shared between different processes.

243
00:35:33,120 --> 00:35:41,120
 And the other important thing is that I am the first address where to map

244
00:35:42,080 --> 00:35:51,840
 this program passes as being zero. This time, the operating system, you are free to choose a region

245
00:35:51,840 --> 00:35:58,480
 of memory in the memory where to map this file. And most of the cases, this is what you are going

246
00:35:58,480 --> 00:36:04,960
 to give. This argument will be zero, right? Because you don't know better than the operating system

247
00:36:04,960 --> 00:36:13,920
 where you should put this file in memory. And my file is a return the starting address

248
00:36:13,920 --> 00:36:19,280
 allocated for that file to be mapped, where the file is mapped to.

249
00:36:19,280 --> 00:36:33,440
 So now, and then you print the file, right? So you pass the file. This is an argument of this

250
00:36:34,080 --> 00:36:44,320
 program and you print it. And then you write something at the address of the file in memory

251
00:36:44,320 --> 00:36:52,000
 plus 10. Let's write over it, okay? So let's see what happens. So first of all, when I print it,

252
00:36:52,000 --> 00:36:57,440
 this is when you print the file. Let's say this is a question. This is the content of the file,

253
00:36:58,080 --> 00:37:05,680
 right? So it's mapped. So the first three lines, so I'm invoking this program with argument test.

254
00:37:05,680 --> 00:37:13,840
 The first three lines are the print-ups I talk about, printing addresses, the data segment,

255
00:37:13,840 --> 00:37:26,720
 heap or stack. And then the next five lines are written by these print-ups. And it's basically

256
00:37:26,720 --> 00:37:32,240
 the content of the test file. So the content of the test files, there are four lines. This is line

257
00:37:32,240 --> 00:37:40,080
 one, line two, line three, and line four. And now here, I'm now this file, every content,

258
00:37:40,080 --> 00:37:45,040
 the content is mapped in memory. And now in memory, like I mentioned, I'm writing at the

259
00:37:45,040 --> 00:37:54,320
 address plus 20. Let's write over it. And then I close the file. And then I want to see what is in

260
00:37:54,320 --> 00:38:01,600
 the file after I close it. And I do cut.test. And this is what I find. This is line one, okay?

261
00:38:01,600 --> 00:38:09,360
 And then this, and then you can see, let's write over it. Because this means that, obviously,

262
00:38:09,360 --> 00:38:23,680
 I wrote, starting from the character 21st, the 21st character in the file, I wrote the string,

263
00:38:23,680 --> 00:38:30,160
 let's write over, which is exactly, and then I close the file. And obviously, everything was saved

264
00:38:30,160 --> 00:38:39,040
 and including the changes in this file. Does it make sense?

265
00:38:39,040 --> 00:38:46,800
 It's pretty cool. You should try it. Okay.

266
00:38:51,520 --> 00:38:59,200
 So, it's like I mentioned, you can share the files between two different processes.

267
00:38:59,200 --> 00:39:04,640
 So here, I have the same file and I have two virtual addresses from two processes.

268
00:39:04,640 --> 00:39:12,320
 And you can share, you can map the file in memory and can be shared this file

269
00:39:12,320 --> 00:39:18,720
 between these two processes. It's exactly like a real file, the file on the disk can be

270
00:39:18,720 --> 00:39:32,480
 shared between different processes. Also, you can use this map to share the data,

271
00:39:32,480 --> 00:39:39,120
 or to share memory and use the memory to share the data between the parent and the children

272
00:39:39,120 --> 00:39:44,560
 and your four children. You can also create an mf file, it's anonymous file, because there is not

273
00:39:44,560 --> 00:39:51,280
 a real file. And you can share to the children and then you can use it to pass data between the

274
00:39:51,280 --> 00:39:58,400
 children and the parents with this anonymous memory mf file. Of course, this is not backed

275
00:39:58,400 --> 00:40:00,480
 by the disk. Okay.

276
00:40:10,720 --> 00:40:17,120
 Yeah, the question here, is a file printed by the printf or the

277
00:40:17,120 --> 00:40:26,880
 foots? So here it's printed by the printf, this file. Puts is a special case

278
00:40:26,880 --> 00:40:38,720
 of printf. With printf, it's formatting, you can print different values in the same instruction.

279
00:40:38,720 --> 00:40:43,280
 So here, you can see the first argument you are going to print.

280
00:40:43,280 --> 00:40:53,760
 Because you are printing the map address and the second argument, you are

281
00:40:53,760 --> 00:41:02,080
 printing what? You are printing what is the address of m file. And there what you have,

282
00:41:02,880 --> 00:41:11,440
 you know, a bunch of, you basically have a bunch of characters, you have a string,

283
00:41:11,440 --> 00:41:15,600
 right? And you bring everything what you find there.

284
00:41:15,600 --> 00:41:19,520
 Okay.

285
00:41:19,520 --> 00:41:26,560
 So,

286
00:41:26,560 --> 00:41:41,440
 so now spans the project three design document, remember is due Saturday, this Saturday, and the

287
00:41:41,440 --> 00:41:55,680
 homework five is due on Monday. Basically, almost two weeks from now. So any questions about memory

288
00:41:55,680 --> 00:42:08,720
 map files? Okay, the next thing we are going to go over is a buffer cache. So remember, we discussed

289
00:42:08,720 --> 00:42:14,880
 that when you read from a file, you will create different copies. And one of these copies is in

290
00:42:14,880 --> 00:42:20,160
 the buffer cache, it's in the operating system can maintain a cache of the data from the file.

291
00:42:20,800 --> 00:42:28,720
 And in this case, you can put the data, you can put inodes, directory, directories, and so forth.

292
00:42:28,720 --> 00:42:35,280
 And some of these pages which are cached can be dirty, obviously. And then the operating

293
00:42:35,280 --> 00:42:40,400
 system has to take care that to write back eventually the dirty pages to the disk.

294
00:42:44,960 --> 00:42:52,640
 And really what you want the key idea, why do you want to cache the disk data into memory again,

295
00:42:52,640 --> 00:42:59,280
 this is an operating system memory, right? It's not in the user space memory, like memory map files,

296
00:42:59,280 --> 00:43:04,960
 you copy directly in the user space for this is in the operating system. So to get this data,

297
00:43:04,960 --> 00:43:09,360
 even if it's in memory, it's in the memory of the operating system, you still need to make a

298
00:43:10,880 --> 00:43:18,480
 system call to get that data. And obviously you cache it to exploit the locality,

299
00:43:18,480 --> 00:43:27,760
 right? There are many accesses to the same blocks. So if you have them in memory, it's much faster

300
00:43:27,760 --> 00:43:34,240
 than accessing the disk. So you need to do this translation between a pass,

301
00:43:35,040 --> 00:43:41,280
 name of the file and the inodes, and then from the block address to the disk contact, right?

302
00:43:41,280 --> 00:43:51,760
 The logical block addresses. So let's see how the buffer cache is working. And here it's setting up.

303
00:43:51,760 --> 00:43:59,840
 So this is on the right hand side, you have the disk. This is like FFS layout on the disk.

304
00:44:00,960 --> 00:44:05,520
 And now this is a buffer cache and the buffer cache has a bunch of blocks.

305
00:44:05,520 --> 00:44:15,200
 And the blocks typically are the same size as pages, memory pages. And then you are going to

306
00:44:15,200 --> 00:44:23,120
 cache the blocks and you can have data blocks, like I mentioned, they can be inodes and they can

307
00:44:23,120 --> 00:44:32,400
 be directory blocks. In Avishon, you have a free, also you can have the free bitmap. Remember,

308
00:44:32,400 --> 00:44:42,240
 you do have the free bitmap on the disk to maintain data about what are the available

309
00:44:42,240 --> 00:44:50,560
 blocks, free blocks. Okay. So this is what you have. These blocks are

310
00:44:51,280 --> 00:44:59,760
 the cache is organized as an array of blocks and the color represent different kinds of types of

311
00:44:59,760 --> 00:45:06,720
 information. And here you have a state, whether in this case is free or dirty or

312
00:45:06,720 --> 00:45:10,480
 things like that, or what happens, whether you read or write a block.

313
00:45:10,480 --> 00:45:19,360
 So let's see what happens. So let's say I am reading an inode, sorry, I am reading a data block.

314
00:45:20,080 --> 00:45:22,880
 So I am reading a data block, a new data block from the disk.

315
00:45:22,880 --> 00:45:28,320
 I am going to find an available block in the cache.

316
00:45:28,320 --> 00:45:36,720
 And then I am going to write, now you see the state is read. This means that now I am

317
00:45:36,720 --> 00:45:44,240
 reading the data from the block on the disk into this available space. Right?

318
00:45:47,280 --> 00:45:58,000
 And then once you read it, you are going to mark that it's no longer free. And if you write it,

319
00:45:58,000 --> 00:46:04,560
 you have to mark it that it's dirty because now this is modified and it's no longer the

320
00:46:04,560 --> 00:46:16,560
 same content as a page on the disk. So now you say you are going to read a

321
00:46:16,560 --> 00:46:23,520
 directory. Right? It's the same thing. You are okay if you look, you find an available block and

322
00:46:23,520 --> 00:46:28,000
 you read it there. Right? And you say what it is, inode.

323
00:46:28,000 --> 00:46:44,480
 Okay. And this is if you do it, the data is the same thing. Right? If you write it,

324
00:46:44,480 --> 00:46:50,000
 now you say dirty, the data is dirty. This is what if you write in the block.

325
00:46:50,000 --> 00:46:59,600
 Right? If you write to a block, you first, if the block is not in the cache, you bring the block in

326
00:46:59,600 --> 00:47:06,160
 the cache and you write in the block on the cache in memory and you mark it as dirty.

327
00:47:06,160 --> 00:47:09,600
 Any questions?

328
00:47:13,120 --> 00:47:18,800
 So discussion. So the buffer cache is entirely implemented in the operating system.

329
00:47:18,800 --> 00:47:29,840
 Right? It's in the software. There is now TLB. Okay. And the blocks go through transitional

330
00:47:29,840 --> 00:47:35,360
 state between free and use, being read from the disk, being written to the disk and so forth.

331
00:47:37,280 --> 00:47:43,600
 And blocks contain all this variable, you know, variety of information like I mentioned,

332
00:47:43,600 --> 00:47:47,280
 inodes and data and directories and the freemap.

333
00:47:47,280 --> 00:47:55,360
 And the OS is going to maintain the management, to maintain the pointer into them.

334
00:47:55,360 --> 00:48:05,520
 It's also the OS is going to manage this buffer cache when the process exits, when writes reads.

335
00:48:06,240 --> 00:48:13,360
 And for instance, on the process exits, you are going to flush to write back all the modified

336
00:48:13,360 --> 00:48:17,920
 data from the buffer cache to the disk.

337
00:48:17,920 --> 00:48:25,600
 Now, this is a cache, it can fill up. So what happens when you fill up? Well, there is a

338
00:48:25,600 --> 00:48:32,000
 replacement policy. Like we know, like demand paging, if you remember. Replacement policy,

339
00:48:32,000 --> 00:48:36,480
 what can you do? You know, you can do allerio. It worked pretty well for demand paging. So I

340
00:48:36,480 --> 00:48:42,480
 want to do something like this. And this works well. And if the memory is big enough, especially

341
00:48:42,480 --> 00:48:47,680
 it can accommodate the working set of the particular

342
00:48:47,680 --> 00:48:54,960
 applications, the working set in terms of the data which is accessed from the file.

343
00:48:59,040 --> 00:49:06,720
 Now, the disadvantage is that it's for some access patterns, allerio is not good.

344
00:49:06,720 --> 00:49:13,920
 So for instance, if I'm just going to scan the file, I'm just going to read

345
00:49:13,920 --> 00:49:17,280
 and to read to need, I'm going to read a block only once.

346
00:49:17,280 --> 00:49:26,720
 So with allerio, that block is going to displace another block in the buffer cache.

347
00:49:26,720 --> 00:49:33,840
 But then the block was loaded into the cache. I am no longer to access again,

348
00:49:33,840 --> 00:49:39,920
 because I got it later. And I'm just doing a scan. So that's a problem.

349
00:49:39,920 --> 00:49:47,200
 So what is the solution here? It's a smart solution. The solution here is that the operating

350
00:49:47,200 --> 00:49:55,360
 system allows applications to request other policies. So the application knows more about

351
00:49:55,360 --> 00:50:01,360
 what is the access pattern. So if the application knows that it's only going to scan the file,

352
00:50:01,360 --> 00:50:10,800
 it can specify as a replacement policy used once. And this means that the file system can discard

353
00:50:10,800 --> 00:50:16,720
 the block as soon as they are used. They don't need to operating system doesn't need to maintain

354
00:50:16,720 --> 00:50:21,040
 that block in the buffer cache.

355
00:50:21,040 --> 00:50:34,640
 Now, the cache is obviously stored in the memory. So one question is that how much memory

356
00:50:34,640 --> 00:50:43,680
 you should give to the operating system for the buffer cache versus the memory for the map,

357
00:50:43,680 --> 00:50:50,240
 the processes virtual address space. If you give too much memory to the file system cache,

358
00:50:50,240 --> 00:50:57,120
 the access can be fast, but you may not have enough room left for the applications.

359
00:50:57,120 --> 00:51:02,640
 If you give too little memory, you are going to have a lot of accesses to the disk,

360
00:51:02,640 --> 00:51:09,120
 because you cannot cache the blocks which are going to be used frequently. So the solution here

361
00:51:09,120 --> 00:51:18,880
 is the operating system adjusts dynamically the boundary between the OS cache and the

362
00:51:18,880 --> 00:51:25,920
 application memory so that the amount of the rate of paging in are roughly the same.

363
00:51:25,920 --> 00:51:30,480
 Okay. Any questions so far?

364
00:51:44,000 --> 00:51:49,760
 Okay. Another thing about the file systems you see a lot is prefetching.

365
00:51:49,760 --> 00:51:55,760
 Prefetching you remember is read ahead. And we discussed that also in the case of demand paging.

366
00:51:55,760 --> 00:52:03,920
 Demand paging is to avoid compulsory misses. If the access is sequential, you know that if you

367
00:52:03,920 --> 00:52:13,200
 have one, if you access one block, then you are going to very likely read the data from the next

368
00:52:13,200 --> 00:52:22,240
 block. So why not bring both blocks right away in memory, right? And avoid to have another access to

369
00:52:22,240 --> 00:52:33,440
 the disk, which is very expensive. Right. Of course, you have a question here, how many,

370
00:52:33,440 --> 00:52:39,760
 you know, how much you are going to read ahead? How many blocks are you going to prefetch?

371
00:52:41,280 --> 00:52:48,320
 If you do too many, then it's fine, but you are going to be wasteful. Right.

372
00:52:48,320 --> 00:52:57,200
 And if you are going to do too little, then you are going to have a lot of more accesses.

373
00:52:57,200 --> 00:53:06,800
 So notice two things though. So one is prefetching is easy if the blocks are

374
00:53:07,760 --> 00:53:14,240
 sequentially arranged on the disk. Right. Because it's one sick time and you just

375
00:53:14,240 --> 00:53:20,160
 read multiple blocks. Right. The other thing is

376
00:53:20,160 --> 00:53:31,680
 if you are going, even if the blocks are not sequential, then you are going to provide

377
00:53:32,320 --> 00:53:40,000
 the disk controller a bunch of blocks you want to read. And you remember the elevator algorithm

378
00:53:40,000 --> 00:53:49,440
 two lectures ago? The elevator algorithms reorder the request, if it gets more requests at the same

379
00:53:49,440 --> 00:53:58,000
 time, so that it uses the sick time. It puts them in the order along the direction that the head is

380
00:53:58,000 --> 00:54:06,320
 moving. Excuse me. So the head doesn't go back and forth. It's just going in one direction and is

381
00:54:06,320 --> 00:54:11,840
 going to get small sick times to the next request, minimizing the sick time.

382
00:54:11,840 --> 00:54:19,600
 So the more prefetching you do, the more requests you are going to give the disk,

383
00:54:19,600 --> 00:54:26,800
 the more potential for the elevator algorithms to be efficient and to minimize the sick time.

384
00:54:26,800 --> 00:54:36,480
 So how much to prefetch? You just discussed. Too much. It's going to be wasteful,

385
00:54:36,480 --> 00:54:41,360
 both because delays, because it takes maybe a little bit more time to prefetch multiple blocks,

386
00:54:41,360 --> 00:54:50,560
 but also because you are going to bring this data in the buffer cache. And maybe you are not going

387
00:54:50,560 --> 00:54:56,640
 to use it. And this data, the blocks you are not going to use it, maybe replaced some blocks you

388
00:54:56,640 --> 00:55:03,280
 are going, which are used frequently. And too little prefetching, many more sick. If you have

389
00:55:03,280 --> 00:55:09,040
 the limit, you prefetch only one, you prefetch nothing. And then you are going to have a sick

390
00:55:09,040 --> 00:55:18,240
 for each new block you are accessing. This was about reading, reading multiple blocks.

391
00:55:19,760 --> 00:55:28,480
 The same you can also about writes. It's called delayed writes. So delay writes basically means

392
00:55:28,480 --> 00:55:36,560
 that it's again, when you write the data and you modify the data, the data is modified only in

393
00:55:36,560 --> 00:55:47,520
 memory in the buffer cache. So it is not written immediately on the disk. So therefore the writes

394
00:55:47,520 --> 00:55:53,360
 are very fast, they are on to memory. The writes to the file are writes to memory, if the block is

395
00:55:53,360 --> 00:56:01,280
 in the buffer cache. The read is going to first go to the buffer cache to see whether the block

396
00:56:01,280 --> 00:56:07,440
 they want to read is in the buffer cache. And if it is in, it's going to get the most up-to-date

397
00:56:07,440 --> 00:56:16,480
 value. Even if that block was not written yet on the disk, it's still the writes, previous write

398
00:56:16,480 --> 00:56:27,040
 updated the block in the memory. So we are fine here. So when does the write, the changes made

399
00:56:27,040 --> 00:56:35,440
 by a write reach the disk? There are a few cases. One is the operating system is flashing it

400
00:56:35,440 --> 00:56:45,440
 periodically. I think like say 30 seconds. Every 30 seconds, you go and scan the buffer cache and

401
00:56:45,440 --> 00:56:56,080
 you look at the blocks which are dirty and write them on the disk. Or obviously, when the cache is

402
00:56:56,080 --> 00:57:04,720
 full, then you may want to flash them to write on the disk. So what are the delay writes advantages?

403
00:57:04,720 --> 00:57:09,040
 Why don't you tell me here? What do you think are the delayed write advantages?

404
00:57:13,040 --> 00:57:26,720
 Compared with you write back every time you modify a block.

405
00:57:26,720 --> 00:57:36,880
 Yes, you do not have to go all the way to the disk for changing every time.

406
00:57:38,080 --> 00:57:43,760
 That's very good. Can you think about anything else?

407
00:57:43,760 --> 00:57:55,920
 So one is, again, if you don't need to go to the disk, you are going to have a performance advantage

408
00:57:55,920 --> 00:57:59,840
 obviously, because you're like I said, you just write to memory, you don't write to the disk.

409
00:58:00,560 --> 00:58:11,040
 It's also you can accumulate multiple writes. You can write to the same block multiple times

410
00:58:11,040 --> 00:58:22,000
 before you are going to write the block to the disk. So you have only one write to the disk

411
00:58:22,000 --> 00:58:29,040
 corresponding to many updates of that block. The other thing is for the reads.

412
00:58:30,000 --> 00:58:35,680
 If you are going to write multiple pages to the disk at the same time, you are going to

413
00:58:35,680 --> 00:58:42,160
 send all these requests to the disk. And the disk, the elevator algorithms on the disk,

414
00:58:42,160 --> 00:58:49,280
 again reorder these requests so that they are going to write close together

415
00:58:49,280 --> 00:58:54,400
 blocks one after another and again minimize the sigma.

416
00:58:54,400 --> 00:59:07,680
 Another thing is that for the layerized, you also may want to allocate multiple blocks at the same

417
00:59:07,680 --> 00:59:19,200
 time for a file to keep them contiguous. You can make room when you write. So you write multiple

418
00:59:19,200 --> 00:59:29,840
 contiguous files. So what I'm trying to say here is that when you are going to write

419
00:59:29,840 --> 00:59:37,120
 data and I have multiple blocks who are modified from or the new blocks, because you can create

420
00:59:37,120 --> 00:59:42,000
 new blocks which are in memory, they are not on the disk, if you expand the file. If you

421
00:59:42,000 --> 00:59:47,200
 write at the end of the file, the file on the disk is smaller. But now you are going to write,

422
00:59:47,200 --> 00:59:52,640
 you are going to allocate more data in the buffer cache. So now the file in memory is larger

423
00:59:52,640 --> 01:00:01,200
 than the file on the disk. So now if I wrote, say, worth of 10 blocks in memory, then if I delay to

424
01:00:01,200 --> 01:00:06,960
 write these 10 blocks to the disk, when I write them, the disk now are these 10 blocks and I'm

425
01:00:06,960 --> 01:00:13,440
 telling them from the same file. So maybe you can find a contiguous place on the disk to place

426
01:00:13,440 --> 01:00:20,720
 all these 10 blocks, one after another, which again is convenient when you read the data from

427
01:00:20,720 --> 01:00:26,080
 these blocks because it's there, you know, one after another. So it's contiguous region of the disk.

428
01:00:26,080 --> 01:00:33,200
 Again, you avoid in that case, we are going to avoid the seek and the rotation like this.

429
01:00:33,200 --> 01:00:43,280
 And here is a fun thing. If you have some files, you open a file during your application,

430
01:00:44,080 --> 01:00:52,160
 and you close it, you just use from some to have a file to create a file to share some data

431
01:00:52,160 --> 01:00:58,320
 between two applications. If that file is very short duration, it may never make on the disk,

432
01:00:58,320 --> 01:01:06,640
 right? Because it's going to be removed and it's going to be deleted, closed, right? And deleted

433
01:01:06,640 --> 01:01:17,520
 before you have the operating system have a chance to write it on the disk. Okay, so here I have a

434
01:01:17,520 --> 01:01:28,400
 question for you. So with demand paging, remember with demand paging, we have an ALERU and the same

435
01:01:28,400 --> 01:01:35,360
 kind of algorithms, but we are very paranoid then about the performance of the ALERU. And we

436
01:01:35,360 --> 01:01:43,360
 implemented some approximations and like, you know, clock algorithms and things like that,

437
01:01:43,360 --> 01:01:50,880
 second chance algorithm. With buffer caching, it turns out that people do not implement exact ALERU

438
01:01:50,880 --> 01:02:00,480
 in the operating system. So people are not as concerned about the performance of ALERU caching

439
01:02:01,280 --> 01:02:08,640
 for buffer caching versus demand paging. Why do you think is that?

440
01:02:08,640 --> 01:02:25,040
 So why do you think that in the case of the buffer caching is fine to implement the exact ALERU,

441
01:02:25,600 --> 01:02:31,440
 while the context of demand paging is not and we need to implement approximation?

442
01:02:31,440 --> 01:02:43,600
 It's in software, so not that much of a performance hit.

443
01:02:43,600 --> 01:02:50,560
 Well, paging is also in software, so demand paging is implemented in software as well.

444
01:02:50,560 --> 01:03:04,400
 Anyone?

445
01:03:04,400 --> 01:03:14,720
 I think it's a subtle question and it's okay, you know, if you do not

446
01:03:17,840 --> 01:03:25,040
 know the answer to this question. So one reason is about, think about the following thing.

447
01:03:25,040 --> 01:03:31,440
 Demand paging, the expectation of the application is going to, you are going to read and write from

448
01:03:31,440 --> 01:03:40,240
 memory. So this is what you are against. You are against in the latency provided by memory because

449
01:03:40,240 --> 01:03:48,480
 that's the expectation of the application, it accesses memory. So you need to be very fast.

450
01:03:48,480 --> 01:03:56,720
 In contrast, when you access a file, you don't have a, you know, expectation to be that fast.

451
01:03:56,720 --> 01:04:05,440
 Buffer cache is improving the latency, but the baseline is accessing the disk, which is slow.

452
01:04:05,440 --> 01:04:10,240
 And sometimes, yeah, you have to access the disk because it's a buffer, you know, that

453
01:04:10,240 --> 01:04:18,880
 the data is not in memory and so forth. So, you know, so that's kind of one reason,

454
01:04:18,880 --> 01:04:26,960
 right? Because the expectation when you access a disk is not to have the fastest

455
01:04:26,960 --> 01:04:34,080
 operations, right? So therefore it's okay if you are, you know, if you are a little bit slower

456
01:04:34,720 --> 01:04:40,880
 and actually with being a little bit slower, you can manage the cache even better, right?

457
01:04:40,880 --> 01:04:43,680
 Because it's no longer an approximation algorithm.

458
01:04:43,680 --> 01:04:54,720
 Eviction policy, demand paging.

459
01:04:54,720 --> 01:05:03,120
 Remember, you have big non-recently used pages when memory is slow, close to full.

460
01:05:04,080 --> 01:05:10,720
 With buffer cache is different. You write back dirty blocks periodically,

461
01:05:10,720 --> 01:05:19,840
 even if used recently, right? And why is that? The reason for that is again, different expectations.

462
01:05:19,840 --> 01:05:25,840
 When I read and write to memory, I don't have any expectation about the ability of the data,

463
01:05:27,040 --> 01:05:33,920
 right? You do expect that the computer crashes or even the program crashes.

464
01:05:33,920 --> 01:05:40,960
 You lose the data in memory, right? But if you write on the disk, if you think that you write

465
01:05:40,960 --> 01:05:49,280
 in a file, you do assume that what you wrote is persistent, right?

466
01:05:50,720 --> 01:05:57,680
 So now, and we'll talk about this quite a bit for the remaining of this lecture and the next lecture.

467
01:05:57,680 --> 01:06:06,960
 You need the operating system to try pretty hard to provide that persistence. So one way

468
01:06:06,960 --> 01:06:16,400
 is going it is by periodically writing the updated modified blocks from the buffer cache to the disk.

469
01:06:16,400 --> 01:06:33,920
 Yes, the marginal difference between ALERIO and non-ALERIO isn't that big in this context,

470
01:06:33,920 --> 01:06:40,880
 since this baseline, since the baseline is a disk operation which takes super long anyways.

471
01:06:41,680 --> 01:06:48,640
 So this is a comment by Gilbert and yeah, it's exactly correct, right? So in the case of the

472
01:06:48,640 --> 01:06:55,440
 buffer cache, the baseline is a disk. In the case of demand paging, the baseline is a memory.

473
01:07:03,120 --> 01:07:14,560
 Okay, so it's like we mentioned, in order to improve the persistency,

474
01:07:14,560 --> 01:07:21,360
 the operating system writes data to the disk from the buffer cache periodically.

475
01:07:21,360 --> 01:07:26,080
 So it flashes it like in the case of Linux every 30 seconds.

476
01:07:28,640 --> 01:07:34,480
 However, obviously this is not bulletproof, right?

477
01:07:34,480 --> 01:07:42,880
 What if the operating system or you have a failure, the machine fails

478
01:07:42,880 --> 01:07:54,080
 and you have dirty blocks into the buffer cache? Obviously, this information is lost,

479
01:07:55,360 --> 01:08:02,240
 right? And now you have the case you have from your application, you wrote something in the file,

480
01:08:02,240 --> 01:08:13,600
 it was open and now machine fails after you wrote. And what you wrote is not in the file.

481
01:08:13,600 --> 01:08:19,520
 Although as a programmer, you may believe that, well, I wrote in a file, the file is persistent,

482
01:08:19,520 --> 01:08:29,440
 it must be in the file because it happened before the failure. And things can be pretty ugly.

483
01:08:29,440 --> 01:08:35,280
 If you, for instance, you only modify a directory, if you don't modify, then you may lose entire

484
01:08:35,280 --> 01:08:41,040
 files because you lost their dangling files, which have no presence in the directory.

485
01:08:44,000 --> 01:08:52,560
 Or there are many others. You may lose inodes and so forth. So the file system needs some

486
01:08:52,560 --> 01:08:58,560
 mechanisms to ensure the persistence, including some recovery mechanisms to recover

487
01:08:58,560 --> 01:09:09,840
 kind of inconsistent state. So there are three important properties of a system when it comes

488
01:09:12,720 --> 01:09:15,920
 to its, so to speak,

489
01:09:15,920 --> 01:09:31,440
 performance availability and so forth of a system and the file system in particular.

490
01:09:31,440 --> 01:09:38,000
 So there are three things. One is availability and availability really tells you

491
01:09:40,160 --> 01:09:46,000
 percentage of time while the system can accept and serve your requests.

492
01:09:46,000 --> 01:09:58,000
 Okay. So three lines, meaning in this case, meaning that the probability you send a request

493
01:09:58,000 --> 01:10:03,360
 to the file system and you don't get a reply or it is not resolved is 0.1%.

494
01:10:05,440 --> 01:10:12,400
 Then durability. The durability means is that if the data is on the disk,

495
01:10:12,400 --> 01:10:21,360
 then it's durable, meaning that if a disk fails, there is still a way to recover the data.

496
01:10:21,360 --> 01:10:30,400
 Availability and durability, they are not the same because availability means that the data

497
01:10:30,400 --> 01:10:38,720
 is somewhere, but doesn't mean necessarily that it's accessible. And then is reliability,

498
01:10:38,720 --> 01:10:45,600
 which is usually the strongest one, is the ability of a system to perform its required functions as

499
01:10:45,600 --> 01:10:52,880
 it is defined and its application level definition, what that means to perform the function.

500
01:10:52,880 --> 01:10:59,440
 And it's typically stronger because it implies availability. If it's not available,

501
01:11:00,000 --> 01:11:13,200
 the system cannot perform its functions and also includes durability because the data has to be,

502
01:11:13,200 --> 01:11:17,520
 any persistent data, should be able to get access to it.

503
01:11:19,840 --> 01:11:30,000
 Okay. So first we are going to look about how to make this system, the file system durable.

504
01:11:30,000 --> 01:11:42,000
 How do you make a file system durable? What I said that what I want to ensure here is that if

505
01:11:42,000 --> 01:11:54,960
 I wrote the data on a disk, in a file, I want to be able to read that data at a later time,

506
01:11:54,960 --> 01:12:00,000
 no matter what, no matter what kind of failures were happening.

507
01:12:00,000 --> 01:12:11,280
 Right? So say the one classic failure is a disk failure. I wrote the data on the disk,

508
01:12:11,280 --> 01:12:17,200
 it made it from the buffer cache to the disk, but then that's dispelled. I wrote the data.

509
01:12:17,200 --> 01:12:23,040
 How do you avoid that?

510
01:12:23,040 --> 01:12:35,680
 Redundancy, you back up. Absolutely. It's what you are doing for your show or you should do for your

511
01:12:36,240 --> 01:12:42,880
 own, the most important data you have. Write to more than one disk, totally.

512
01:12:42,880 --> 01:12:58,000
 Okay. And there are many ways to do it. And we'll see, you can do it, read Solomon,

513
01:12:59,360 --> 01:13:06,800
 you're correcting codes. There are other things you can do. It is like you can have battery backed

514
01:13:06,800 --> 01:13:12,880
 RAMs or non-volatile RAM. So it doesn't necessarily need to be on the disk, but you can also write it

515
01:13:12,880 --> 01:13:21,600
 in RAM as long as it's powered as in the own battery, so when the machine fails, the RAM is

516
01:13:21,600 --> 01:13:32,560
 not wiped out. But to really to have the data surviving more, a lot of more long term,

517
01:13:32,560 --> 01:13:37,760
 we need to replicate it. And again, when you replicate it, you need to be very clear about the

518
01:13:37,760 --> 01:13:46,160
 assumptions, failure assumption you are making. Because if you have two disk drives

519
01:13:46,160 --> 01:13:54,640
 and both are on the same machine, if the machine fails, then you lost both copies.

520
01:13:54,640 --> 01:14:02,400
 To avoid that, you need to put copies on different machines. But if the machines are in the same rack

521
01:14:02,400 --> 01:14:08,480
 and the rack fails or the data center experienced a catastrophic failure,

522
01:14:09,920 --> 01:14:16,480
 you lost access to those copies. Now you can put the copies on several different continents,

523
01:14:16,480 --> 01:14:23,120
 you are going to be safer there. And you see that, right? Because typically, many of you assume

524
01:14:23,120 --> 01:14:29,600
 that when they back up their data, where do you back up the data? Do you back on the same machine

525
01:14:29,600 --> 01:14:40,400
 now? Where do you back it up? Actually, I'm curious. Anyone back up the data?

526
01:14:40,400 --> 01:14:51,520
 Yes, cloud. Many people bring their phone to clone the cloud.

527
01:14:53,120 --> 01:15:02,880
 Yeah, and Michael has a good point. Maybe also as a medium,

528
01:15:02,880 --> 01:15:08,640
 you want to write on different media to protect against some catastrophic failure,

529
01:15:08,640 --> 01:15:13,600
 which is going to involve only on media or degradation over time.

530
01:15:13,600 --> 01:15:22,560
 But many people write to the cloud. And the cloud actually can replicate the data

531
01:15:22,560 --> 01:15:25,520
 across different regions in different continents.

532
01:15:25,520 --> 01:15:35,440
 But now, getting back, and this was many years back, and this was

533
01:15:35,440 --> 01:15:41,200
 people who developed first the technology called RAID, actually developed at Berkeley

534
01:15:41,200 --> 01:15:50,800
 by David Patterson, Randy Katz and their student Gar Gibson, who was faculty at Carnegie Mellon,

535
01:15:50,800 --> 01:16:00,080
 now it's at Toronto. And you just have a replica for each disk and one you call recovery

536
01:16:00,080 --> 01:16:12,160
 group. And you double the capacity you need. And when you write, write is done only if you write

537
01:16:12,160 --> 01:16:21,200
 both at the primary and the recovery group. And that's it, right?

538
01:16:21,200 --> 01:16:27,280
 Now, the problem here is that the writes can be quite expensive because each write

539
01:16:27,280 --> 01:16:37,680
 it needs to go to two disks. And you remember when you write, you have rotation disk latency.

540
01:16:38,400 --> 01:16:45,760
 Well, you are going now to finish and you are going to wait for the slowest one to finish disk,

541
01:16:45,760 --> 01:16:53,280
 for the slowest disk to finish. That was a performance is worse for writes. Of course,

542
01:16:53,280 --> 01:17:00,000
 you can synchronize the disks, right? So then all both of them finish at the same time.

543
01:17:00,000 --> 01:17:01,920
 But that's very hard.

544
01:17:04,720 --> 01:17:10,880
 What about the reads? Well, it turns out that with reads you are better, right? Because

545
01:17:10,880 --> 01:17:16,560
 with reads you can actually, you can have twice the bandwidth because you can read for

546
01:17:16,560 --> 01:17:27,120
 any replica. Right? Or you can send the same read to both replicas and you get the one which is

547
01:17:27,120 --> 01:17:35,440
 arrived first. So it's good for reads, both in terms of latency and the throughput is bad for

548
01:17:35,440 --> 01:17:45,760
 writes. And also you need to double the capacity. What happens on the recovery when a disk fails?

549
01:17:45,760 --> 01:17:50,320
 You need to buy a new disk and then you are going to copy the data to the new disk.

550
01:17:50,320 --> 01:17:55,440
 Write all data to the new disk. So now you have again two replicas.

551
01:17:55,440 --> 01:18:05,520
 Now that was again pretty expensive. And that was RAID 1. Again, RAID

552
01:18:05,520 --> 01:18:18,800
 means redundant arrays of inexpensive disks, as opposed to much more expensive, presumably more

553
01:18:18,800 --> 01:18:24,800
 reliable disks. Here you have cheap, less reliable disks, but because you do this kind of replication

554
01:18:24,800 --> 01:18:31,520
 or now you see razor coding, you are going to achieve a much higher reliability, given a much

555
01:18:31,520 --> 01:18:41,040
 higher reliability than an expensive disk. So, and there are different again levels of RAID.

556
01:18:41,040 --> 01:18:51,120
 And you can look in the textbook. But RAID 5 and more, what they do, they basically,

557
01:18:52,240 --> 01:19:03,360
 they stripe the data across multiple disks and they add also a parity disk. A parity block.

558
01:19:03,360 --> 01:19:15,520
 OK, so basically say in this case, you know, in this figure that each D, D1, D2, D3, these are

559
01:19:17,360 --> 01:19:30,800
 disks and these are the blocks which you could, so D0, D1, D2, D3, sorry, these are the blocks,

560
01:19:30,800 --> 01:19:37,600
 the name of the blocks, D0, D1, D2, D3. And then to this one, you are going to add a parity block,

561
01:19:38,240 --> 01:19:48,640
 P0. And the one you can do it, the P0 is the XOR of the values of the data blocks. Right?

562
01:19:48,640 --> 01:19:55,200
 So therefore, you have four data blocks, you are going to add another one, which is a parity one.

563
01:19:55,200 --> 01:20:04,720
 So the overhead here is 25%, not doubled. Right? And what happens here, you can, if one disk goes

564
01:20:04,720 --> 01:20:13,760
 away, you can still reconstruct the data. And how do you do it? You reconstruct the data of the lost

565
01:20:13,760 --> 01:20:22,240
 block by simple XORing. All the remaining blocks, the three remaining blocks, plus the parity block.

566
01:20:24,240 --> 01:20:36,240
 It's as simple as that. Okay? And you can also spread the information not only across multiple

567
01:20:36,240 --> 01:20:41,840
 disks in the same server or in the same data center, you can also do it across the internet.

568
01:20:41,840 --> 01:20:51,280
 Right? So you can use the same technique. Okay, so I'm going to stop here. We are going to talk

569
01:20:51,280 --> 01:21:04,560
 a little bit more about this, how to make this durable. And next time. Okay, thank you. And this

570
01:21:04,560 --> 01:21:11,440
 Thursday, as you know, it's Veterans Day. So I hope that you are going to enjoy a well-deserved

571
01:21:13,360 --> 01:21:30,640
 lighter day. And you can take, you can get some rest. Okay, I'll see you next week. Bye.

